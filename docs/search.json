[
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Explore my portfolio",
    "section": "",
    "text": "I’m passionate about turning data into beautiful, meaningful visuals that tell a story that helps connect knowledge to action.\n\n\n\n\n  \n\nX →\n\n\n\n \n\nThis is a terrain map of Colorado with a custom color scheme, ranging from yellow, green, to light blue for the highest elevation. I used a subtle 3-d, textured styling for this map, which is inspired by John Nelson’s pan oblique tutorial. Designed in ArcGIS Pro.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a map of the Upper Colorado River Basin, with the Yampa River, a tributary of the Colorado, highlighted in the upper-right hand corner. I created this map to be used in a series of presentations and other communications regarding conservation projects led by Yampa Valley Sustainability Council. Designed in ArcGIS Pro.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a topographic map of the Upper Yampa River Basin with a contour interval of 100ft. Designed in QGIS.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a map of Routt County, Colorado. This was designed to be used in presentations and other communications for Yampa Valley Sustainability Council. Designed in ArcGIS Pro.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a map of weather and climate monitoring stations in the Upper Yampa River Basin. It was designed for Yampa Valley Sustainability Council in support of their Y-Basin Monitoring Network design. Designed In ArcGIS Pro.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a map of the average total annual precipitation in Northwestern Colorado from 1991 - 2020. Precipitation values range from orange (8 inches per year) to dark blue (72 inches per year). It was created using data from PRISM Climate Group at Oregon State University. Designed in ArcGIS Pro. .\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a map of California Counties shaded by their percentile rankings for average PM 2.5 levels compared to national values. Darker orange indicates areas in the highest percentiles (80-100%), while lighter yellow represents those in the lowest percentiles (0-20%). I created this map using the US EPA’s EJ Screen Data. Designed in R-Studio.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis map of California Counties is shaded by the percentage of the households with low income. Darker purple indicates areas in the highest percentages (40-50% of households), while lighter purple represents those with lower percentage (10-20% of households). I created this map using the US EPA’s EJ Screen Data. Designed in R-Studio.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis map of Tulare County shows census tracts shaded by the percentage of the population that identifies as people of color. Darker blue indicates areas in the highest percentages, while lighter blue represents those with lower percentage. I created this map using the US EPA’s EJ Screen Data. Designed in R-Studio.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis map of Tulare County shows census tracts shaded by their percentile rankings for cancer risk compared to national values. Darker green indicates areas in the highest percentiles (80-100%), while lighter green represents those in the lowest percentiles (0-20%). I created this map using the US EPA’s EJ Screen Data. Designed in R-Studio.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThe above map displays the Home Owners’ Loan Corporation (HOLC) Grades, also known as redlining, in Los Angeles, California. Historically, these boundaries were used by mortgage lenders to identify ‘hazardous’ or high risk areas predominately made up of black and minority populations, as well as low income areas, to avoid for mortgage lending. Practices like redlining are a clear illustration of systemic racism and the enduring legacy of slavery. Impacts from redlining are still being felt today. Designed In R-Studio.\n\n\n\n  \n\nX ←\n\n\n\n \n\nThis animation shows a map of Houston’s night light intensity on a typical night before the storm (February 7, 2023), compared to night light intensity after the storm (Feb 16, 2023), when the Houston region in Texas experienced widespread power outages. The night light intensity ranges from no light (dark blue), to pink (mid light intensity), to yellow (highest light intensity). Designed In R-Studio.\n\n\n\n\n\n\n\n\n  \n\nX →\n\n\n\n \n\nThis boxplot shows the distribution of natural hazard risk exposure (x-axis) by county for states in the continental U.S. (y-axis).FEMA’s NRI risk scores run from 0 to 100 with increasing severity. This plot reveals that California counties face the greatest natural hazard risk in the United States. With more than 75% of its counties falling above relatively moderate risk and more than 50% of counties falling in the high risk category. Created in R.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis stacked bar plot shows the proportion of natural hazard risk exposure (x-axis) by race and ethnicity (y-axis) in California. While almost all counties across California are exposed to moderate to high risk, this plot highlights that the distribution of risk is not uniform across race. A higher proportion of certain race classes, including those who identify as Black/African American and Asian classes, are especially exposed to the highest risk class. Important to note, this plot only considers risk for counties in California with a population greater than 65,000. Created in R.\n\n\n\n  \n\n← X\n\n\n\n \n\nThis graph highlights Snow Water Equivalent data for the Tower SNOTEL station, data provided by USDA NRCS. The graph was created for Yampa Valley Sustainability Council. Created in Microsoft Excel."
  },
  {
    "objectID": "portfolio.html#maps",
    "href": "portfolio.html#maps",
    "title": "Explore my portfolio",
    "section": "",
    "text": "X →\n\n\n\n \n\nThis is a terrain map of Colorado with a custom color scheme, ranging from yellow, green, to light blue for the highest elevation. I used a subtle 3-d, textured styling for this map, which is inspired by John Nelson’s pan oblique tutorial. Designed in ArcGIS Pro.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a map of the Upper Colorado River Basin, with the Yampa River, a tributary of the Colorado, highlighted in the upper-right hand corner. I created this map to be used in a series of presentations and other communications regarding conservation projects led by Yampa Valley Sustainability Council. Designed in ArcGIS Pro.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a topographic map of the Upper Yampa River Basin with a contour interval of 100ft. Designed in QGIS.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a map of Routt County, Colorado. This was designed to be used in presentations and other communications for Yampa Valley Sustainability Council. Designed in ArcGIS Pro.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a map of weather and climate monitoring stations in the Upper Yampa River Basin. It was designed for Yampa Valley Sustainability Council in support of their Y-Basin Monitoring Network design. Designed In ArcGIS Pro.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a map of the average total annual precipitation in Northwestern Colorado from 1991 - 2020. Precipitation values range from orange (8 inches per year) to dark blue (72 inches per year). It was created using data from PRISM Climate Group at Oregon State University. Designed in ArcGIS Pro. .\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is a map of California Counties shaded by their percentile rankings for average PM 2.5 levels compared to national values. Darker orange indicates areas in the highest percentiles (80-100%), while lighter yellow represents those in the lowest percentiles (0-20%). I created this map using the US EPA’s EJ Screen Data. Designed in R-Studio.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis map of California Counties is shaded by the percentage of the households with low income. Darker purple indicates areas in the highest percentages (40-50% of households), while lighter purple represents those with lower percentage (10-20% of households). I created this map using the US EPA’s EJ Screen Data. Designed in R-Studio.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis map of Tulare County shows census tracts shaded by the percentage of the population that identifies as people of color. Darker blue indicates areas in the highest percentages, while lighter blue represents those with lower percentage. I created this map using the US EPA’s EJ Screen Data. Designed in R-Studio.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis map of Tulare County shows census tracts shaded by their percentile rankings for cancer risk compared to national values. Darker green indicates areas in the highest percentiles (80-100%), while lighter green represents those in the lowest percentiles (0-20%). I created this map using the US EPA’s EJ Screen Data. Designed in R-Studio.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThe above map displays the Home Owners’ Loan Corporation (HOLC) Grades, also known as redlining, in Los Angeles, California. Historically, these boundaries were used by mortgage lenders to identify ‘hazardous’ or high risk areas predominately made up of black and minority populations, as well as low income areas, to avoid for mortgage lending. Practices like redlining are a clear illustration of systemic racism and the enduring legacy of slavery. Impacts from redlining are still being felt today. Designed In R-Studio.\n\n\n\n  \n\nX ←\n\n\n\n \n\nThis animation shows a map of Houston’s night light intensity on a typical night before the storm (February 7, 2023), compared to night light intensity after the storm (Feb 16, 2023), when the Houston region in Texas experienced widespread power outages. The night light intensity ranges from no light (dark blue), to pink (mid light intensity), to yellow (highest light intensity). Designed In R-Studio."
  },
  {
    "objectID": "portfolio.html#data-viz",
    "href": "portfolio.html#data-viz",
    "title": "Explore my portfolio",
    "section": "",
    "text": "X →\n\n\n\n \n\nThis boxplot shows the distribution of natural hazard risk exposure (x-axis) by county for states in the continental U.S. (y-axis).FEMA’s NRI risk scores run from 0 to 100 with increasing severity. This plot reveals that California counties face the greatest natural hazard risk in the United States. With more than 75% of its counties falling above relatively moderate risk and more than 50% of counties falling in the high risk category. Created in R.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis stacked bar plot shows the proportion of natural hazard risk exposure (x-axis) by race and ethnicity (y-axis) in California. While almost all counties across California are exposed to moderate to high risk, this plot highlights that the distribution of risk is not uniform across race. A higher proportion of certain race classes, including those who identify as Black/African American and Asian classes, are especially exposed to the highest risk class. Important to note, this plot only considers risk for counties in California with a population greater than 65,000. Created in R.\n\n\n\n  \n\n← X\n\n\n\n \n\nThis graph highlights Snow Water Equivalent data for the Tower SNOTEL station, data provided by USDA NRCS. The graph was created for Yampa Valley Sustainability Council. Created in Microsoft Excel."
  },
  {
    "objectID": "portfolio.html#infographics",
    "href": "portfolio.html#infographics",
    "title": "Explore my portfolio",
    "section": "Infographics",
    "text": "Infographics\n\n\n  \n\nX →\n\n\n\n \n\nThis is an infographic that provides an overview of the benefits of healthy riparian areas and provides three simple tips for how homeowners can help protect and enhance them. I created this to support a riparian health initiative by Yampa Valley Sustainability Council. Designed in Adobe Illustrator and Canva.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is an infographic that provides an overview of the benefits of healthy riparian areas and provides three simple tips for how homeowners can help protect and enhance them. I created this to support a riparian health initiative by Yampa Valley Sustainability Council. Designed in Adobe Illustrator and Canva.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is an infographic that provides an overview of the benefits of planting trees along rivers. I created this to support an initiative of the Yampa River Forest Restoration Project led by Yampa Valley Sustainability Council. Designed in Adobe Illustrator.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is an infographic that provides an overview of the 2022 - 2026 Strategic Priorities for Yampa Valley Sustainability Council. I designed this to support their 5-year strategic planning process. Designed in Adobe Illustrator.\n\n\n\n  \n\nX ← →\n\n\n\n \n\nThis is an infographic that provides an overview of the datasets that were included in a spatial statistical analysis to support the Y-Basin Integrated Network project led by Yampa Valley Sustainability Council. Designed in Adobe Illustrator."
  },
  {
    "objectID": "portfolio.html#videos",
    "href": "portfolio.html#videos",
    "title": "Explore my portfolio",
    "section": "Videos",
    "text": "Videos"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Some text goes here"
  },
  {
    "objectID": "about.html#what-i-do-for-work",
    "href": "about.html#what-i-do-for-work",
    "title": "About",
    "section": "",
    "text": "Some text goes here"
  },
  {
    "objectID": "about.html#what-i-do-for-fun",
    "href": "about.html#what-i-do-for-fun",
    "title": "About",
    "section": "What I do for fun",
    "text": "What I do for fun\nSome text goes here\nhere is some text\nhere is some more text"
  },
  {
    "objectID": "about.html#what-i-do-for-fun-1",
    "href": "about.html#what-i-do-for-fun-1",
    "title": "About",
    "section": "What I do for fun",
    "text": "What I do for fun\nSome text goes here"
  },
  {
    "objectID": "blogs/2024-12-12-co-avalanches/index.html",
    "href": "blogs/2024-12-12-co-avalanches/index.html",
    "title": "A Look At Elevated Risk Factors for Avalanche Accidents in Colorado",
    "section": "",
    "text": "There is something wondrous about getting yourself up a snowy mountain, beit by the power of a snowmobile or skis underfoot - surrounded by nothing but peaks, trees, and your trusty companions beside. Winter recreation can be really fun, exciting, and uniquely challenging. With advancements in touring and navigating equipment (and cool photos on social media), it’s no surprise that backcountry winter adventures are more popular than ever.\nAs an avid snowboarder, I enjoy exploring the backcountry on skis or by snowmobile. It wasn’t until I moved from California to a mountain town in Northern Colorado, just over 6 years ago, that I fully realized the significant risks that avalanches play in the mountains. Colorado is the most dangerous state for avalanche risk, leading the U.S. in avalanche accidents year to year. Its steep, rugged terrain and cold temperatures, mixed with low-moisture snowstorms creates conditions that make “the perfect storm” for avalanche conditions. These conditions make it harder for snowpack to consolidate, leaving unstable slabs of new snow, sitting on top of weak, faceted older layers, just waiting for a slight trigger to set them in motion. There are a lot of factors that influence avalanche risk, so I wanted to investigate that with my project.\nColorado is fortunate to have the Colorado Avalanche Information Center(CAIC), which is an organization that provides avalanche education, research, and daily forecasting for the public. They have a robust avalanche reporting and forecasting system. Below is a feature of their interactive dashboard that provides daily avalanche risk forecasting.  Check out the CAIC website\nThis is where I got the avalanche report data for my project. Their readily available dataset has only a subset of information that CAIC collects for their avalanche reports and only contains fatal accidents, excluding avalanche accidents where a human was involved (but no fatality occurred). This limited the type of question that I could ask for this assignment, given the timeline.\nI know that CAIC has a danger scale that ranges from low to extreme and they actually, and they designate different risk levels for three elevation bands: below ( &lt; 11,000ft), near (11,000 - 12,000 ft), and above (12,000ft + ). This is because various environmental factors (like temperature, wind, precipitation) influence how snow consolidates at these levels. Knowing that elevation plays an important role in how CAIC designates and communicates avalanche risk, I designed my research question around how elevation impacts the risk of an avalanche accident.\nThe model formation that I ended up landing on was:"
  },
  {
    "objectID": "blogs/2024-12-12-co-avalanches/index.html#set-up-workspace",
    "href": "blogs/2024-12-12-co-avalanches/index.html#set-up-workspace",
    "title": "A Look At Elevated Risk Factors for Avalanche Accidents in Colorado",
    "section": "1. Set Up Workspace",
    "text": "1. Set Up Workspace\nTo get started, I need to set up my workspace. In the code chunk below, I load in the packages that will be necessary for my analysis.\n\n# ---- Load required packages ----\n\nlibrary(here) # for file management\nlibrary(tidyverse) # for data wrangling\nlibrary(ggplot2) # for plotting\nlibrary(knitr) # for tables and visualizing things\nlibrary(kableExtra) # for creating tables\nlibrary(DT) # for creating df tables\nlibrary(broom) # for creating table for glm results\nlibrary(terra) # for working with vector spatial data\nlibrary(sf) # for working with vector spatial data\nlibrary(elevatr) # for extracting elevation at points"
  },
  {
    "objectID": "blogs/2024-12-12-co-avalanches/index.html#read-in-project-data",
    "href": "blogs/2024-12-12-co-avalanches/index.html#read-in-project-data",
    "title": "A Look At Elevated Risk Factors for Avalanche Accidents in Colorado",
    "section": "2. Read in Project Data",
    "text": "2. Read in Project Data\nNext up, I read in CAIC’s avalanche accident data, filtered for the state of Colorado. The data is made up of 271 avalanche observations from 1951 - 2023. A table of the project data is below.\n\n# ---- Read in data ----\nav_accidents &lt;- read_csv(here::here(\"data\",\"avalanche_accidents.csv\")) %&gt;%\n  filter(State == \"CO\") # Filter for Colorado (CO) in State Column\n\n# Create table for CO avalanche data\ndatatable(av_accidents, options = list(pageLength = 3))"
  },
  {
    "objectID": "blogs/2024-12-12-co-avalanches/index.html#prepare-data",
    "href": "blogs/2024-12-12-co-avalanches/index.html#prepare-data",
    "title": "A Look At Elevated Risk Factors for Avalanche Accidents in Colorado",
    "section": "3. Prepare Data",
    "text": "3. Prepare Data\nBefore I can extract elevation values for the accident locations, I need to convert the av_accidents data to a spatial object. In the next code chunk, I clean lat and lon columns by removing ‘NA’ and obscure values including values ‘0’ and ‘#REF!’, then I convert the data frame to a spatial object using the sf packages st_as_sf tool.\n\nConvert av_accidents to a Spatial Object\n\n# ---- Clean av_accidents data ----\n\n# Clean lat and lon columns by removing NAs, 0, and #REF values\nav_accidents &lt;- av_accidents %&gt;% \n    drop_na(lat, lon) %&gt;% # Drop NAs in lat and lon columns\n    mutate(across(c(lat, lon), as.character)) %&gt;%  # Ensure columns are characters for filtering\n    filter(lat != \"0\", lon != \"0\", lat != \"#REF!\", lon != \"#REF!\") %&gt;% # Remove values that equal 0 or #REF!\n    mutate(across(c(lat, lon), as.numeric))  # Convert back to numeric\n\n# Convert avalanche accidents to spatial object to extract elevation at geometry\nav_points &lt;- sf::st_as_sf(av_accidents, coords = c(\"lon\", \"lat\"), crs = 4326) # Use 4326 to match Elevatr data crs\n\n\n\nExtract & Prepare Elevation at Accident Locations\nTo get the elevation at accident locations I use the get_elev_point tool from the elevatr package. This tool accesses USGS elevation from the Amazon Warehouse Service API to extract elevation at points without having to download a whole raster.\nThen I add a column for elevation_group, because for my project I specifically want to look at avalanche risk relative to the different elevation bands that CAIC forecasts for using the values in the table below for “Below”, “Near” and “Above” treeline.\n\n\n\n\n\n\n\n\n\nElevation Band\nMin Elevation (m)\nMax Elevation (m)\n\n\n\n\n\nBelow Treeline (&lt; 11,000ft)\n\n3,352\n\n\n\nNear Treeline (11,000ft &lt; 12,000ft)\n3,352\n3,652\n\n\n\nAbove Treeline ( 12,000ft &lt; )\n3,652\n\n\n\n\n\n\n# ---- Extract elevation at avalanche accident locations ----\n\n# Call in elevation data using `elevatr`\nav_points &lt;- get_elev_point(locations = av_points,\n                            prj = sf::st_crs(4326)$proj4string)\n\n# Convert the result back to a data frame by replacing av_accidents\nav_accidents &lt;- as.data.frame(av_points)\n\n# ---- Clean and remove unwanted elevation values ---- \n\nav_accidents &lt;- av_accidents %&gt;% \n    filter(elevation != 0, elevation != -10000) \n\n# ---- Add a column for elevation band  ---- \n\n# Define and add a column for elevation groups\nav_accidents &lt;- av_accidents %&gt;%\n  mutate(elevation_group = case_when(\n    elevation &lt; 3352.8 ~ \"Below Treeline\",\n    elevation &gt;= 3352.8 & elevation &lt;= 3652 ~ \"Near Treeline\",\n    elevation &gt; 3652 ~ \"Above Treeline\"\n  ))\n\n\n\nTake a Look at Accidents by Elevation\n\n\nCode\n# ---- Plot accidents by elevation group ----\n\nggplot(data = av_accidents) + \n  geom_histogram(aes(x = elevation),\n                fill = \"#4ECBDE\",\n                alpha = 0.8) +\ngeom_vline(xintercept = c(3352.8, 3652), color = \"firebrick\", size = 0.75, linetype = \"solid\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 16),\n       axis.title.x = element_text(angle = 0,\n                                   vjust = 0.5,\n                                   face = \"bold\"),\n       axis.title.y = element_text(angle = 0,\n                                   vjust = 0.5,\n                                   face = \"bold\")) +\n  labs(title = \"Avalanches Accidents by Elevation Zone in Colorado\",\n       x = \"Elevation (Meters)\",\n       y = \"Avalanche\\nAccidents\") +\n  geom_text(aes(x = 3100, y = .5), \n           label = \"Below Treeline\", size = 4, fontface = \"italic\") +\n  geom_text(aes(x = 3500, y = .82), \n           label = \"Near\\nTreeline\", size = 4, fontface = \"italic\") +\n  geom_text(aes(x = 3850, y = .5), \n           label = \"Above Treeline\", size = 4, fontface = \"italic\")\n\n\n\n\n\n\n\n\n\n\nPrepare the Month Data for Avalanche Accidents\nNext, I want to prepare the month data for my model. I’d like to start by creating a preliminary graph to see the distribution of avalanche accidents by month. Before I can do that, I want to create a month names column and assign their respective names. To do this, I define vector lists for month_labels (for their names) and month_levels to reorder them sequentially for the winter season. Then I create a new column in the av_accidents dataset as an ordered factor; I set the labels equal to month_labels, the levels as month_levels, and the ordered parameter to TRUE.\n\n# ---- Prepare month column for preliminary plots ----\n\n# Define month labels and order\nmonth_levels &lt;- c(9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8)\nmonth_labels &lt;- c(\"September\", \"October\", \"November\", \"December\", \n                  \"January\", \"February\", \"March\", \"April\", \n                  \"May\", \"June\", \"July\", \"August\")\n\nav_accidents$month_names &lt;- factor(av_accidents$MM, \n                                        levels = month_levels, \n                                        labels = month_labels, \n                                        ordered = TRUE)\n\nNow it’s time to visualize avalanche accidents by month in Colorado.\n\n\nCode\n# ---- Plot accidents by month sorted by season order ----\nggplot(data = av_accidents) + \n  geom_bar(aes(x = month_names),\n                fill = \"#4ECBDE\",\n                alpha = 0.8)  +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\",size = 16),\n        axis.text.x = element_text(angle = 0,\n                                   hjust = 0.5,\n                                   face = \"bold\"),\n        axis.title.y = element_text(angle = 0,\n                                    vjust = 0.5,\n                                    face = \"bold\"),\n        axis.title.x = element_text(angle = 0,\n                                    face = \"bold\")) +\n  geom_text(\n    aes(x = factor(month_names), label = ..count..),\n    stat = \"count\",                        \n    vjust = -0.5,\n    color = \"firebrick\",\n    fontface = \"bold\"\n  ) +\n  labs(title = \"Avalanche Accidents in Colorado by Month\",\n       x = \"Month\",\n       y = \"Fatal\\nAccidents\")\n\n\n\n\n\n\n\n\n\nThis plot above is a bar chart with month_names on the x-axis and the count of fatal accidents on the y-axis for Colorado from 1950 - 2023. Based on the plot, it looks like there are fewer fatal avalanches in November and May than in the middle winter months, like February and March. This data shows that March has observed the most avalanche accidents, with a total of 35 fatal accidents.\nLooking at the distribution of accidents across months, it is more of a curved than a linear relationship between month and fatal accidents. Considering that I’m going to be performing a generalized linear model (glm), it would be beneficial to transform my data in a way that will fit be more linear. Since March is the month with the highest number of accidents, I’m going to create a new column for my dataset based off of the months ‘distance’ (months) to March.\n\n\n\n\n\n\n\n\n\nMonth Name\nMonth Number ‘MM’\nDistance to March (months)\n\n\n\n\n\nJanuary\n1\n2\n\n\n\nFebruary\n2\n1\n\n\n\nMarch\n3\n0\n\n\n\nApril\n4\n1\n\n\n\nMay\n5\n2\n\n\n\nJune\n6\n3\n\n\n\nJuly\n7\n4\n\n\n\nAugust\n8\n5\n\n\n\nSeptember\n9\n6\n\n\n\nOctober\n10\n5\n\n\n\nNovember\n11\n4\n\n\n\nDecember\n12\n3\n\n\n\n\nIn the code chunk below, I create a key to assign a new month column MM_to_March based on their closest distance (months) to March.\n\n# ---- Prepare the MM_to_March column for glm ----\n\nmonth_distance_to_march &lt;- c(\n  \"1\" = 2, \"2\" = 1, \"3\" = 0,\n  \"4\" = 1, \"5\" = 2, \"6\" = 3,\n  \"7\" = 4, \"8\" = 5, \"9\" = 5,\n  \"10\" = 4, \"11\" = 4, \"12\" = 3\n)\n\n# Apply the MM column to distances relative to March\nav_accidents$MM_to_March &lt;- as.numeric(month_distance_to_march[as.character(av_accidents$MM)])"
  },
  {
    "objectID": "blogs/2024-12-12-co-avalanches/index.html#fit-the-generalized-linear-model-for-avalanche-accidents",
    "href": "blogs/2024-12-12-co-avalanches/index.html#fit-the-generalized-linear-model-for-avalanche-accidents",
    "title": "A Look At Elevated Risk Factors for Avalanche Accidents in Colorado",
    "section": "4. Fit the Generalized Linear Model for Avalanche Accidents",
    "text": "4. Fit the Generalized Linear Model for Avalanche Accidents\nThe question that I’m exploring with my generalized linear model is How does month and elevation zone and their interaction influence the probability of a fatal avalanche accident? Before I perform the glm, I need to group and aggregate my data by the variables that I will including in the model: MM_to_March and elevation_group.\nA reminder that MM_to_March is a continuous variable for distance (months) to March, since March was the month with the most amount of avalanche accidents. Additionally, elevation_group, which will be treated as a categorical variable, designates which of the three major elevation bands, that CAIC categorizes for avalanche risk, the avalanche accidents occurred in.\nAlright, so now that I have my data all prepped, I’m ready to set up my model:\nThe model formulation is: ##### Avalanche Accidents ~ MM_to_March + elevation_group + MM_to_March : elevation_group\nThe family of model that I ended up using was Poisson(link = “log”). This is a standard family for modeling count data, which assumes that the response variable follows a Poisson distribution with a mean that is a function of the predictors. The log link is commonly used for this family because it ensures the predicted count remains positive.\n\n# ---- Define the glm for avalanche accidents ----\n\n# Aggregate data by Month and elevation group\nav_accidents_agg &lt;- av_accidents %&gt;%\n  group_by(MM_to_March, elevation_group) %&gt;%\n  summarise(accidents = n(), .groups = \"drop\")\n\n\n# Fit the avalanche accident model\nav_accident_glm &lt;- glm(accidents ~ elevation_group + MM_to_March + elevation_group:MM_to_March, \n             family = poisson(link = \"log\"), \n             data = av_accidents_agg)"
  },
  {
    "objectID": "blogs/2024-12-12-co-avalanches/index.html#explore-model-results",
    "href": "blogs/2024-12-12-co-avalanches/index.html#explore-model-results",
    "title": "A Look At Elevated Risk Factors for Avalanche Accidents in Colorado",
    "section": "5. Explore Model Results",
    "text": "5. Explore Model Results\nIn the code chunk below I explore my model results.\n\n# Explore model results\nsummary(av_accident_glm)\n\n\nCall:\nglm(formula = accidents ~ elevation_group + MM_to_March + elevation_group:MM_to_March, \n    family = poisson(link = \"log\"), data = av_accidents_agg)\n\nCoefficients:\n                                          Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                                2.16171    0.29739   7.269 3.62e-13\nelevation_groupBelow Treeline              0.90758    0.35059   2.589  0.00963\nelevation_groupNear Treeline               0.59954    0.36502   1.642  0.10049\nMM_to_March                               -0.24403    0.18341  -1.330  0.18336\nelevation_groupBelow Treeline:MM_to_March -0.14839    0.21036  -0.705  0.48056\nelevation_groupNear Treeline:MM_to_March  -0.04744    0.21269  -0.223  0.82351\n                                             \n(Intercept)                               ***\nelevation_groupBelow Treeline             ** \nelevation_groupNear Treeline                 \nMM_to_March                                  \nelevation_groupBelow Treeline:MM_to_March    \nelevation_groupNear Treeline:MM_to_March     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 48.317  on 13  degrees of freedom\nResidual deviance: 16.027  on  8  degrees of freedom\nAIC: 81.934\n\nNumber of Fisher Scoring iterations: 5\n\n# ---- Make a nice table for GLM results ----\n\n# Tidy the GLM results\nglm_summary &lt;- tidy(av_accident_glm)\n\n# Create using kable\nkable(glm_summary, digits = 3, caption = \"GLM Coefficients for Avalanche Accidents\")\n\n\nGLM Coefficients for Avalanche Accidents\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.162\n0.297\n7.269\n0.000\n\n\nelevation_groupBelow Treeline\n0.908\n0.351\n2.589\n0.010\n\n\nelevation_groupNear Treeline\n0.600\n0.365\n1.642\n0.100\n\n\nMM_to_March\n-0.244\n0.183\n-1.330\n0.183\n\n\nelevation_groupBelow Treeline:MM_to_March\n-0.148\n0.210\n-0.705\n0.481\n\n\nelevation_groupNear Treeline:MM_to_March\n-0.047\n0.213\n-0.223\n0.824\n\n\n\n\n\nLet’s dig into the details:\nCoefficients for elevation:\n\nThe intercept or Above Tree Line group for my model, which is when the month is March (MM_to_March = 0) at reference group set to “Above Tree Line”, returned a coefficient as 2.16 and a significant p-value. Given the significance of the p-value, this suggests that being “Above Tree Line” is a significant predictor for my model.\nThe Below Treeline group for my model provided a coefficient of 0.98 and a significant p-value of 0.01. This suggests that accidents are significantly more likely to happen in the “Below Treeline” group.\nThe Near Treeline group for my model returned an estimate of 0.6 and an insignificant p-value of 0.1, which provides evidence that being in the “Near Treeline” group does not meaningfully influence the likelihood of a fatal avalanche accident.\n\nCoefficients for month:\n\nThe MM_to_March group returned a coefficient of -0.244 which suggests that as you move away from March there are fewer fatal accidents, however the p-value of 0.183 suggests that this relationship is not significant.\n\nCoefficients for interaction term:\n\nNon of the interaction terms had a significant p-value, which provides evidence that the effect of elevation does not vary significantly by month.\n\n\nOverall conclusion\nThe model results show that, for my avalanche data, distance from March and the interaction between distance from March and elevation group are not significant predictors for avalanche accidents. This provides evidence that these factors alone do not substantially affect the avalanche accident counts. However, the model did provide evidence that elevation group, particularly that being below treeline positively influence the likelihood of a fatal avalanche accident occurring.\nTo me this result is likely influenced by the reality that more people recreate closer to trailheads, which typically are closer or below treeline and less people travel further into the backcountry, rather than an environmental factor like the model was hoping to get at. The preliminary plot showing avalanche count by elevation showed that the residuals after 4000 m are very negative, which supports my assumption, so this could certainly be impacting the results of my model.\nSome important things that I want to point out are that my data didn’t fit a linear pattern very well, which is important for using glms. So I don’t think that the question that I asked for the data was ideal. Avalanches are seasonal and I believe that there are better ways for modeling this type of data that I’m not yet familiar with.\nAdditionally, avalanche fatalities have an important behavioral factor that was difficult for me to model with this data, and given the scope of this project, so I think that incorporating that could be interesting in the future. It would also be be cool to investigate the proportion of fatal vs non-fatal accidents. This would require me finding additional data from CAIC or another group.\nThis analysis certainly raises more questions than answers, but I thoroughly enjoyed going through the process of defining a research question, finding data, following my intuition, and designing a model.\n\n\nReferences:\n\nColorado Avalanche Information Center (CAIC), CAIC Accident Data 2023 (https://avalanche.state.co.us/accidents/statistics-and-reporting) Access date: 12/01/24\nUnited States Geologic Survey (USGS), The National Map - Elevation Point Query Service.(https://apps.nationalmap.gov/epqs/) Accessed using Elevator package in R. Access date: 12/01/24\n\n\n\nAcknowledgments:\nThis assignment was created for UCSB MEDS, EDS 223 - Statistics for Environmental Data Science. Thank you to our professor Max Czapanskiy and teaching assistant Leo Feitosa for their wisdom and support throughout the class.\n\n\nLink to GitHub Repo:\nhttps://github.com/nicolelpepper/co-avalanche-accidents"
  },
  {
    "objectID": "blogs/2025-03-25-an-upstream-battle/index.html",
    "href": "blogs/2025-03-25-an-upstream-battle/index.html",
    "title": "An Upstream Battle For Winter-Run Chinook in California",
    "section": "",
    "text": "The Upstream Battle\nSalmon populations in California have faced significant challenges over the past century. Today, the State’s remaining salmon populations continue to battle environmental challenges introduced by diversions and dams, built throughout the 1890s to 1960s, which have led to decreased flows, decreased water quality, and increased water temperatures.\nWinter-Run Chinook salmon, found only in California’s Upper Sacramento River Valley, have been particularly hit hard by these environmental challenges because they uniquely spawn during the hot summer months when river and ambient temperatures are often at their hottest. Cool river temperatures, below 53.5°F, are critical for the successful maturation and survival of their eggs.\nEvery year from November to August, winter-run Chinook, embark on an upstream journey from the San Francisco Bay to the upper Sacramento River Valley. Today, salmon travel as far north as Keswick Dam, which completely blocks access to their historic spawning grounds. In the early 1990s winter-run Chinook narrowly escaped extinction. Their persistent battle to survive has inspired my infographic, “An Upstream Battle for Winter-Run Chinook in California”.\n\n\n\n\n\n\n\nThe Data\n\nOverviewMapDonut ChartsStream ChartHeatmapSoftware\n\n\nExplore the following tabs to learn about the data sources used for this visualization.\n\n\n\nGeospatial Data:\n\nCalifornia State Boundary [Shapefile]. Retrieved from California Department of Education (Accessed April 2025).\n\nSacramento-San Joaquin River Delta Boundary [Shapefile]. Retrieved from NOAA Fisheries Salmon Critical Habitat Database (Accessed April 2025).\n\nSacramento River [Shapefile]. Retrieved from NOAA Fisheries Salmon Critical Habitat Database (Accessed April 2025).\n\nWinter-run ESU Chinook Salmon Habitat Boundary [Shapefile]. Retrieved from NOAA & CDFW (Accessed April 2025).\n\nKeswick Dam [Shapefile]. Retrieved from California Open Data Portal i17 California Jurisdictional Dams Dataset (Accessed April 2025).\n\n\n\n\n\nPercent Habitat Donut Chart:\nHabitat Loss Data [Webpage]. Retrieved from NOAA Fisheries (Accessed April 2025).\n- Quote reference: “While many factors have affected the salmonid populations in California’s Central Valley, a driving factor is the presence of large dams. As dams have been constructed over the past century, as much as 95% of the salmonid spawning habitat has been lost.” – NOAA Fisheries (Accessed April 2025).\n\n\nPercent Population Chart:\nSalmon Population Data [Dataset]. Retrieved from The Nature Conservancy (Accessed April 2025).\n- Dataset provides annual estimates of adult population size for various salmon species, including winter-run Chinook, across California.\n- Calculation based on adult winter-run Chinook salmon population estimates from 2023 compared to 1970.\n\n\n\n\nPrimary Dataset:\nSalmon Population Data [Dataset]. Retrieved from The Nature Conservancy (Accessed April 2025).\n- This dataset provides annual estimates of adult population size for various salmon species, including winter-run Chinook, across California.\n\n\nAnnotations:\nBattle Creek Restoration Project [Webpage]. Retrieved from California Department of Fish and Wildlife (Accessed April 2025).\n\n\n\nStream Temperature Data California Department of Water Resources. Kewsick Site Stream Temperature Data [Dataset]. Retrieved from California Data Exchange Center (Accessed April 2025).\n\nKeswick Dam is a critical spawning location for Chinook, since it is now the furthest north reach that is accessible for fish. The dataset provides hourly stream temperature readings from 2020 - present.\n\n\n\n\nProgramming Language\n\nR\n\n\n\nSoftware\n\nR-Studio: For data manipulation and creating graphic elements\nAffinity: For final layout and graphic design\n\n\n\n\n\n\n\nThe Design Choices\n\nThemeColorsTypographyAccessibility\n\n\nAll of the design elements for my infographic are inspired by the resilience of salmon and rivers. I aimed for a style that was engaging, informative, and approachable - while balancing the seriousness of the issue. Explore the following tabs to learn more about my design choices.\nExplore the following tabs to learn more about my design choices.\n\n\n For my color palette, I chose a combination of blue and pink. Blue serves as a universal representation of water, I selected the shades based on an image of a river in Northern California. The pink is inspired by the distinctive reddish-pink hue that male Chinook salmon develop during spawning, though I opted for a more vibrant hot pink to enhance visual interest. I also prioritized accessibility by selecting color hues and saturation levels that are colorblind-friendly.\n\n\n\n\n\n\n\n\nHeader\nI chose Brief River, by Green Adventure Studio, for my headers because I liked how it was chunky and nostalgic. Being a serif font and having thicker, more bubbly letters, I felt like it was a good fit for my fish & river-theme. I selected Li Gothic for my body text because I liked how it was minimal, balanced, and legible.\n\n\nBody Text\nI chose Brief River, by Green Adventure Studio, for my headers and Apple Li Gothic, by Apple, for my body text. I selected Brief River because I liked how it was chunky and nastalgic. Being a serif font, I felt like it was a great fit for my river-themed graphic. I selected Li Gothic for my body text because I liked how it was minimal, balanced, and legible.\n\n\n\n\nColorblind-Friendly\nI use a colorblind-friendly palette with high contrast colors so that the visualization is accessible to those with color vision deficiencies.\n\n\nAlt Text\nI include descriptive alt text so screen readers can accurately convey the content to visually impaired users.\n\n\nLabels & Annotations\nI place labels directly on charts and highlight key takeaways with text to improve readability and comprehension.\n\n\n\n\n\n\nThe Design Process\nI designed this infographic around the central themes of salmon and rivers. One of the biggest challenges was balancing the complexity of the story and historical context without overwhelming the viewer. Through several iterations, I crafted a visual narrative, guiding the viewers naturally from one element to the next building a clear and cohesive message.\nThe infographic is structured to read from the top to bottom, with the title, description, two donut charts, and map at the top to provide critical context about the precarious state of winter-run Chinook salmon. At the center is a filled-area plot depicting winter-run Chinook population from the 1970s to 2023. The dramatic decline in population, immediately stood out to me, and given the theme of the piece, I was inspired to shape it like a flowing river by using a filled blue area. To ensure that the reader took away context regarding the decline in population, I pointed out key historic events using text annotations including when the population dropped below 200 in the early 1990s, a temporary rebound in population when there was a 5-year period increased delta release flows, and the Battle Creek Restoration Project which released approximately 20K juvenile Chinook into Battle Creek, a tributary of the Sacramento River. To further reinforce the connection between dams and reduced flows, I incorporated an illustration of a dam next to the y-axis, making it appear as though the graph represents water flowing out of the dam.\nOne of the most challenging elements to integrate was the river temperature data, a critical factor affecting salmon survival. I used stream temperature data from California Department of Water Resources at the California Data Exchange Center for a site right below Kewsick Dam, which is now a critical spawning location for Chinook, since it is the furthest north reach that is accessible for fish. The dataset provides hourly stream temperature readings from 2020 - present. I experimented with several variations of heatmaps. Initially, I felt like the continuous heatmap displaying a color associated with the max daily temperature (the style that is in my final infographic) was too detailed, so I simplified the graph by aggregating and summarizing the days that days-per-month that exceeded the temperature threshold of 53.3°F in a calendar-style heatmap.\n\n\n\nCalendar-style heatmap.\n\n\nHowever, even though the calendar-style graph looked simpler it was more difficult to interpret and lost key details due to aggregation. After discussing it with my peers, I ultimately decided to revert to the continuous heatmap, which conveyed the data more effectively. To improve the readability, I spaced out the bars for the months and matched the colors palette with the rest of the theme.\nMy goal for the infographic is to raise awareness about the challenges that salmon in Northern California face, particularly for winter-run Chinook salmon, and to inspire community engagement and approval for conservation initiatives that support this critical species. My intended audience for the piece are the general public, community stakeholders, conservation and fisheries managers, as well as environmental advocates.\n\n\nThe Code:\nBelow is the code for used to build the individual elements in my infographic. All of the graphic elements were created in R. After designing the graphs, I exported them as .pdfs imported them into Affinity for final formatting and to incorporate custom illustrations into the final piece.\n\n\nLoad Libraries\n# ---- Load libraries ----\n\n# Data wrangling  \nlibrary(here)  \nlibrary(dplyr)  \nlibrary(tidyverse)  \nlibrary(janitor)  \nlibrary(readxl)  \n\n\n# Spatial data processing  \nlibrary(sf)  \nlibrary(smoothr)  \n\n# Data visualization  \nlibrary(ggplot2)  \nlibrary(ggstream)  \nlibrary(scales)\n\n\n\n\nLoad Data\n# ---- Read in Salmon population data ----\nsalmon_ca &lt;- read_excel(here::here(\"data\",\"upstream\",\"State_of_Salmon_in_CA_083024.xlsx\")) |&gt; \n    clean_names()\n\n\n# ---- Read in Chinook Habitat map data ----\nca_state &lt;- st_read(here::here(\"data\", \"upstream\", \"California_State_Boundary\", \"California_State_Boundary.shp\")) |&gt;\n    smooth(method = \"chaikin\") # smooth() to simplify geometries\nriver &lt;- st_read(here::here(\"data\", \"upstream\", \"river\", \"sac_river.shp\")) |&gt;\n    st_union() |&gt;\n    smooth(method = \"chaikin\")\nbay &lt;- st_read(here::here(\"data\", \"upstream\", \"river\", \"sac_bay_wr_chinook.shp\")) |&gt; smooth(method = \"chaikin\")\nhabitat &lt;- st_read(here::here(\"data\",\"upstream\", \"river\", \"salmon_habitat.shp\")) |&gt; smooth(method = \"chaikin\")\ndams &lt;- st_read(here::here(\"data\", \"upstream\", \"river\", \"chinook_dams.shp\"))\n\n\n# ---- Read in Sacramento River temp data for Keswick Dam ----\nkes_stream_temp &lt;- read.csv(here::here(\"data\",\"upstream\", \"KWK_25_w_temp.csv\")) |&gt;\n  clean_names() |&gt;\n  mutate(obs_date = as.Date(obs_date),\n         day_of_year = yday(obs_date),\n         year = year(obs_date),\n         month = month(obs_date, label = TRUE))|&gt;\n  filter(year &lt; 2025)\n\n\n\n\nDefine Color Palettes\n# Define color palettes\ntemp_palette &lt;- c(\"#004564\",\"#65D8FE\", \"#FC90A9\",\"#E11847\")\n\nprimary_palette &lt;- c(\n  \"ocean\" = \"#004564\",\n  \"river\" = \"#20566E\",\n  \"cascade\" = \"#65D8FE\",\n  \"salmon\" = \"#FC90A9\",\n  \"crimson\" = \"#E11847\",\n  \"light-blue\" = \"#D5FFFD\",\n  \"white\" = \"white\"\n)\n\n\n\nThe Map\n\n\nClean Data\n# Transform CRS to match\nca_state &lt;- st_transform(ca_state, st_crs(4326))\ndams &lt;- st_transform(dams, st_crs(4326)) |&gt; filter(NAME == \"Keswick\")\nhabitat &lt;- st_transform(habitat, st_crs(4326))\nbay &lt;- st_transform(bay, st_crs(4326)) |&gt; st_make_valid()\nriver &lt;- st_transform(river, st_crs(4326)) \n\n\n\n\nCreate Habitat Map\n# ---- Create a map of Chinook Salmon Habitat in Northern California ---\n\nggplot() +\n# Load CA outline\n  geom_sf(data = ca_state,\n          fill = primary_palette[\"river\"],\n          color = NA) + \n  \n  # Load chinook habitat boundaries\n  geom_sf(data = habitat,\n          aes(fill = Class),\n          fill = c(primary_palette[\"salmon\"],\n                   primary_palette[\"cascade\"]),\n          color = c(alpha(primary_palette[\"salmon\"], 0.6),\n                    alpha(primary_palette[\"cascade\"], 0.6)),\n          alpha = 0.3) + \n  \n  # Load Sacramento River\n  geom_sf(data = river,\n          color = primary_palette[\"cascade\"]) +\n  \n  # Load SF Bay\n  geom_sf(data = bay,\n          fill = primary_palette[\"ocean\"],\n          color = primary_palette[\"ocean\"]) +\n  \n  # Load Keswick Dam\n  geom_sf(data = dams,\n          color = primary_palette[\"crimson\"]) +\n    \n  # Add river annotation\n  annotate(\"text\",\n            x = -121.5, y = 39.2,\n           label = \"Sacramento River\",\n           color = primary_palette[\"cascade\"],\n           fontface = \"bold\",\n           size = 2.5) +\n  \n  # Define Albers Equal Area proj for California\n  coord_sf(crs = st_crs(3311)) +\n  \n  # Define theme\n  theme_void() + \n  theme(\n    plot.background = element_rect(fill = primary_palette[\"ocean\"])\n  )\n\n\n\n\n\n\n\n\n\nThe Donut Charts\n\n\nCreate Donut Chart of Chinook Population\n# Define chinook population proportion, based on 2023 and 1970 pop estimates\npop_prop &lt;- data.frame(\n  category = c(\"Remaining\", \"Historic Pop\"),\n  value = c(5, 95)\n)\n\n# Create the donut chart\nggplot(pop_prop, aes(x = 2,  # Set x to a constant value\n                     y = value,\n                     fill = category)) +\n  \n  geom_bar(stat = \"identity\",\n           width = 1,\n           color = \"white\") +  # Bar borders\n  \n  coord_polar(\"y\", start = 0) +  # Transform to polar coordinates\n  \n  # Define the width of the donut\n    xlim(0.2,\n       2.5) +\n  \n  scale_fill_manual(values = c( \"#F94B74\", \"#65D8FE\")) +\n  \n  labs(title = \"Only 5% of Historic Population Remains\") +\n  \n  theme_void() +\n  \n  theme(\n    plot.title = element_text(face = \"bold\",\n                              color = primary_palette[\"white\"],\n                              size = 16,\n                              hjust = 0.5),\n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    plot.background = element_rect(fill = primary_palette[\"ocean\"],\n                                   color = NA),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\nCreate Donut Chart of Access to Historic Spawning Grounds\n# Define habitat proportion, based estimates from CalFish and Boydstun 2001\npop_prop &lt;- data.frame(\n  category = c(\"Remaining\", \"Historic Pop\"),\n  value = c(10, 90)\n)\n\n# Create the donut chart\nggplot(pop_prop, aes(x = 2,\n                     y = value,\n                     fill = category)) +\n    \n  geom_bar(stat = \"identity\",\n           width = 1,\n           color = \"white\") +\n    \n  coord_polar(\"y\", start = 0) +\n\n  # Define width and height of chart\n  xlim(0.2,\n       2.5) +\n    \n  scale_fill_manual(values = c( \"#F94B74\", \"#65D8FE\")) +\n    \n  labs(title = \"90% of historic spawning habitat\\nis no longer accessible\") +\n    \n  theme_void() +\n\n  theme(\n    plot.title = element_text(face = \"bold\",\n                              color = primary_palette[\"white\"],\n                              size = 16,\n                              hjust = 0.5),\n    \n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    plot.background = element_rect(fill = primary_palette[\"ocean\"],\n                                   color = NA),\n    legend.position = \"none\"\n  ) \n\n\n\n\n\n\n\n\n\nThe Stream Chart\n\n\nPrep Data\n# Clean and filter data for winter-run chinook\nwinter_salmon &lt;- salmon_ca |&gt;\n    filter(c_name == \"Chinook\") |&gt;\n    filter(r_timing == \"Winter-run Chinook\") |&gt;\n    dplyr::group_by(y_end,\n                    r_timing) |&gt;\n    dplyr::summarise(abun_estimate = sum(abun_estimate, na.rm = TRUE),\n                     .groups = \"drop\") \n\n\n\n\nCreate Stream Chart\n# --- Filled stream chart for winter-run salmon abundance ----\n\n# Define plot\nggplot(data = winter_salmon,\n       aes(x = y_end,\n           y = abun_estimate,\n           fill = r_timing)) +\n    \n    # Add stream geometry for salmon abundance over time\n    geom_stream(type = \"ridge\") +\n    \n    # Define color scheme\n    scale_fill_manual(values = \"#65D8FE\") +\n    \n    # Label y axis with K for thousands\n    scale_y_continuous(labels = scales::label_number(scale = 1e-3,\n                                                     suffix = \"K\"),\n                        breaks = c(10e3, 30e3, 50e3),\n                        expand = c(0, 0)) +\n  \n    # Edit x axis labels and breaks\n    scale_x_continuous(breaks = seq(1970, 2030, by = 10),\n                       expand = c(0,0)) +\n    \n    # ---- Add annotations ----\n    \n    # 1994 label for pop below 200 ----\n            # Add red arrow for 1994 - population less than 200 ----\n            annotate(\"text\",\n                     x = 1994, y = 5000, \n                     label = \"↓\",\n                     color = primary_palette[\"salmon\"],\n                     size = 1) +\n            \n            # Add population text annotation\n            annotate(\"text\", x = 1994, y = 11500, \n                     label = \"1994\\nPopulation Drops\\nto less than 200\",\n                     color = primary_palette[\"white\"],\n                     size = 3,\n                     fontface = \"bold\") +\n            \n            # Add endangered text annotation\n            annotate(\"text\", x = 1993, y = 8000, \n                     label = \"Put on the endangered\\n species list\",\n                     color = primary_palette[\"white\"],\n                     size = 2.5) +\n  \n      # Add 2000 - 2005 bracket for increased delta release ----\n            # Add bracket from 2000 - 2005 ----\n            geom_segment(aes(x = 2000, xend = 2000,\n                             y = 9500, yend = 11000),\n                         color = primary_palette[\"cascade\"],\n                         size = .25,\n                         linetype = \"dashed\") +\n            \n            geom_segment(aes(x = 2005, xend = 2005,\n                             y = 9500, yend = 11000),\n                         color = primary_palette[\"cascade\"],\n                         size = .25,\n                         linetype = \"dashed\") +\n            \n            geom_segment(aes(x = 2000, xend = 2005,\n                             y = 11000, yend = 11000),\n                         color = primary_palette[\"cascade\"],\n                         size = .25,\n                         linetype = \"dashed\") +  \n  \n            # add text annotation for 20% increase\n            annotate(\"text\",\n                       x = 2002.5, y = 13500, \n                       label = \"2000-2005\\n20% Increase\\nin Delta Release\",\n                       color = primary_palette[\"white\"],\n                       size = 2.5) +\n  \n      # Add 2018 label for battle creek ----\n             # Add arrow\n              geom_segment(aes(x = 2018, xend = 2018,\n                             y = 0, yend = 7500),\n                         color = primary_palette[\"cascade\"],\n                         size = .25,\n                         linetype = \"dashed\") +\n            \n            # Add annotation for battle creek\n            annotate(\"text\",\n                     x = 2018, y = 10000, \n                     label = \"2018\\n20k juvenile Chinook\\nreleased in Battle Creek\",\n                     color = primary_palette[\"white\"],\n                     size = 2.5) +\n  \n   # Add 2021 label for eggs lost ----\n             # Add arrow\n              geom_segment(aes(x = 2021, xend = 2021,\n                             y = 0, yend = 17000),\n                         color = primary_palette[\"cascade\"],\n                         size = .25,\n                         linetype = \"dashed\") +\n            \n            # Add annotation for battle creek\n            annotate(\"text\",\n                     x = 2020, y = 20000, \n                     label = \"2021\\nAn estimated 75%\\nof eggs were lost due\\nto high river temperatures\",\n                     color = primary_palette[\"white\"],\n                     size = 2.5) +\n    \n    labs(\n        title = \"In the 90s winter-run Chinook narrowly avoided extinction\",\n        y = \"Estimated\\nWinter-Run\\nChinook\\nAdult\\nPopulation\",\n        fill = \"Return Type\") +\n    \n  # Define theme\n  theme_minimal() +\n  theme(\n        plot.title = element_text(face = \"bold\",\n                                  color = primary_palette[\"white\"],\n                                  size = 12),\n        axis.text.x = element_text(size = 10,\n                                   face = \"bold\",\n                                   color = primary_palette[\"white\"]),\n        axis.text.y = element_text(size = 10,\n                                   face = \"bold\",\n                                   color = primary_palette[\"white\"]),\n        axis.title.y = element_text(size = 11,\n                                    face = \"bold\",\n                                    color = primary_palette[\"white\"],\n                                    angle = 0,\n                                    vjust = .5),\n        axis.title.x = element_blank(),\n        panel.grid = element_blank(),\n        axis.line.x = element_line(color = primary_palette[\"white\"], size = 0.5),\n        axis.ticks.x = element_line(color = primary_palette[\"white\"], size = 0.5),\n        axis.ticks.y = element_blank(),\n        plot.background = element_rect(fill = primary_palette[\"ocean\"],\n                                       color = NA),\n        legend.position = \"none\"\n        ) \n\n\n\n\n\n\n\n\n\nThe Heatmap\n\n\nPrep Data\n# ---- Calculate average and max daily stream temperature ----\ndaily_avg_temp_kes20 &lt;- kes_stream_temp |&gt;\n  group_by(day_of_year,\n           year) |&gt;\n  summarise(avg_temp = mean(value, na.rm = TRUE),\n            max_temp = max(value, na.rm = TRUE)) |&gt;\n  ungroup()\n\n\n\n\nCreate Heatmap\n# ---- Create heatmap of Keswick stream temperature by day of year ----\nggplot(daily_avg_temp_kes20,\n       aes(x = day_of_year,\n           y = factor(year), # facet by year\n           fill = max_temp)) +\n    \n  geom_tile() +\n    \n coord_cartesian(clip = \"off\") +\n     \n  # Add a box around April 15 to August 31\n  geom_rect(aes(xmin = as.numeric(strptime(\"04-15\", \"%m-%d\")$yday),\n                xmax = as.numeric(strptime(\"08-31\", \"%m-%d\")$yday),\n                ymin = -Inf,\n                ymax = Inf),\n            color = primary_palette[\"light-blue\"],\n            fill = NA,\n            size = 1.5) +\n    \n  # Add a small rectangle below the spawning season dates\n  geom_rect(aes(xmin = as.numeric(strptime(\"04-15\", \"%m-%d\")$yday),\n                xmax = as.numeric(strptime(\"08-31\", \"%m-%d\")$yday),\n                ymin = 0,\n                ymax = .4),\n            fill = primary_palette[\"light-blue\"],\n            color = primary_palette[\"light-blue\"],\n            size = 1) +\n    \n  # Label spawning season\n  geom_text(aes(x = (as.numeric(strptime(\"04-15\", \"%m-%d\")$yday + 1) + \n                     as.numeric(strptime(\"08-31\", \"%m-%d\")$yday + 1)) / 2,\n                y = 0.25,\n                label = \"Spawning Season: Apr. 1 - Aug. 31\"),\n            color = primary_palette[\"ocean\"],\n            fontface = \"bold\",\n            size = 3) +\n    \n    scale_fill_gradientn(colors = temp_palette) +\n    scale_y_discrete(limits = rev(levels(factor(daily_avg_temp_kes20$year)))) +  \n    \n  labs(title = \"Daily maximum river temperature below Keswick Dam.\") +\n  theme_void() +\n  theme(plot.title = element_text(color = primary_palette[\"white\"],\n                                  face = \"bold\",\n                                  size = 16),\n        axis.text.y = element_text(color = primary_palette[\"white\"],\n                                   face = \"bold\",\n                                   size = 20), \n        axis.ticks.y = element_blank(),\n        plot.background = element_rect(fill = primary_palette[\"ocean\"],\n                                       color = NA),\n        legend.text = element_text(color = primary_palette[\"white\"],\n                                   size = 12),\n        legend.title = element_blank(),\n        legend.position = \"top\",\n        legend.justification = c(1, 1))\n\n\n\n\n\n\n\n\n\nThe Stacked Bar Chart\n\n\nPrep Data\n# ---- Calculate days exceeding the threshold of 53.5°F ----\n\n# Define threshold\nthreshold &lt;- 53.5 \n\n# Flag days where any hourly observation exceeds the threshold\ndaily_exceedance &lt;- kes_stream_temp |&gt;\n  group_by(year, month, obs_date) |&gt;\n  summarise(exceed_threshold = as.integer(any(value &gt; threshold,\n                                              na.rm = TRUE))) |&gt;\n  ungroup()\n\n# ---- Filter data for spawning season (April - August) ----\nspawning_exceedance &lt;- daily_exceedance |&gt;\n  filter(month %in% c(\"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\")) |&gt; \n  group_by(year) |&gt;\n  summarise(exceed_days = sum(exceed_threshold),\n            total_days = n()) |&gt;\n  mutate(non_exceed_days = total_days - exceed_days) |&gt;\n  pivot_longer(cols = c(exceed_days, non_exceed_days), \n               names_to = \"exceed_type\", \n               values_to = \"days\") |&gt;\n  mutate(proportion = days / total_days)\n\n\n\n\nCreate Stacked Bar Chart\n# ---- Stacked bar chart showing proportion per year that exceeded or didn't ----\nggplot(spawning_exceedance, aes(y = factor(year,\n                                           levels = rev(unique(year))), \n                                x = proportion, \n                                fill = exceed_type)) +\n  geom_bar(stat = \"identity\",\n           color = primary_palette[\"white\"],\n           size = 0.5) +\n  \n  # Add labels for percentage\n  geom_text(aes(label = ifelse(round(proportion * 100) &gt;= 15, \n                               paste0(round(proportion * 100), \"%\"), \n                               \"\")),\n            position = position_stack(vjust = 0.5), \n            color = primary_palette[\"white\"], \n            fontface = \"bold\",\n            size = 12) +\n  \n  scale_fill_manual(name = \"Temperature Status\", \n                    values = c(\"exceed_days\" = \"#F94B74\", \n                               \"non_exceed_days\" = \"#20566E\")) +\n \n  scale_x_continuous(labels = scales::percent) +\n\n  labs(title = \"Percent of spawning season days that met and exceeded the threshold\") +\n    \n  theme_minimal() +\n    \n  theme(plot.title = element_text(face = \"bold\",\n                                  color = primary_palette[\"white\"],\n                                  size = 20,\n                                  hjust = 0.5),\n        \n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        panel.grid = element_blank(),\n        axis.ticks = element_blank(),\n        plot.background = element_rect(fill = primary_palette[\"ocean\"],\n                                       color = NA),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThe Full Code\n\n\nExpand to See Full Code\n# ---- Load libraries ----\n\n# Data wrangling  \nlibrary(here)  \nlibrary(dplyr)  \nlibrary(tidyverse)  \nlibrary(janitor)  \nlibrary(readxl)  \n\n# Spatial data processing  \nlibrary(sf)  \nlibrary(smoothr)  \n\n# Data visualization  \nlibrary(ggplot2)  \nlibrary(ggstream)  \nlibrary(scales)\n\n# ---- Read in data ----\n\n# Read in Salmon population data\nsalmon_ca &lt;- read_excel(here::here(\"data\",\"upstream\",\"State_of_Salmon_in_CA_083024.xlsx\")) |&gt; \n    clean_names()\n\n\n# Read in Chinook Habitat map data\nca_state &lt;- st_read(here::here(\"data\", \"upstream\", \"California_State_Boundary\", \"California_State_Boundary.shp\")) |&gt; smooth(method = \"chaikin\") # smooth() to simplify geometries\nriver &lt;- st_read(here::here(\"data\", \"upstream\", \"river\", \"sac_river.shp\")) |&gt; st_union() |&gt; smooth(method = \"chaikin\")\nbay &lt;- st_read(here::here(\"data\", \"upstream\", \"river\", \"sac_bay_wr_chinook.shp\")) |&gt; smooth(method = \"chaikin\")\nhabitat &lt;- st_read(here::here(\"data\",\"upstream\", \"river\", \"salmon_habitat.shp\")) |&gt; smooth(method = \"chaikin\")\ndams &lt;- st_read(here::here(\"data\", \"upstream\", \"river\", \"chinook_dams.shp\"))\n\n\n# Read in Sacramento River temp data for Keswick Dam\nkes_stream_temp &lt;- read.csv(here::here(\"data\",\"upstream\", \"KWK_25_w_temp.csv\")) |&gt;\n  clean_names() |&gt;\n  mutate(obs_date = as.Date(obs_date),\n         day_of_year = yday(obs_date),\n         year = year(obs_date),\n         month = month(obs_date, label = TRUE))|&gt;\n  filter(year &lt; 2025)\n\n# ---- Define color palettes ----\n\n# Define heatmap palette\ntemp_palette &lt;- c(\"#004564\",\"#65D8FE\", \"#FC90A9\",\"#E11847\")\n\n# Define primary palette\nprimary_palette &lt;- c(\n  \"ocean\" = \"#004564\",\n  \"river\" = \"#20566E\",\n  \"cascade\" = \"#65D8FE\",\n  \"salmon\" = \"#FC90A9\",\n  \"crimson\" = \"#E11847\",\n  \"light-blue\" = \"#D5FFFD\",\n  \"white\" = \"white\"\n)\n\n\n# ---- Create The Map ----\n\n# Transform CRS to match\nca_state &lt;- st_transform(ca_state, st_crs(4326))\ndams &lt;- st_transform(dams, st_crs(4326)) |&gt; filter(NAME == \"Keswick\")\nhabitat &lt;- st_transform(habitat, st_crs(4326))\nbay &lt;- st_transform(bay, st_crs(4326)) |&gt; st_make_valid()\nriver &lt;- st_transform(river, st_crs(4326)) \n\n# Create map\nggplot() +\n  # Load CA outline\n  geom_sf(data = ca_state,\n          fill = primary_palette[\"river\"],\n          color = NA) + \n  \n  # Load chinook habitat boundaries\n  geom_sf(data = habitat,\n          aes(fill = Class),\n          fill = c(primary_palette[\"salmon\"],\n                   primary_palette[\"cascade\"]),\n          color = c(alpha(primary_palette[\"salmon\"], 0.6),\n                    alpha(primary_palette[\"cascade\"], 0.6)),\n          alpha = 0.3) + \n  \n  # Load Sacramento River\n  geom_sf(data = river,\n          color = primary_palette[\"cascade\"]) +\n  \n  # Load SF Bay\n  geom_sf(data = bay,\n          fill = primary_palette[\"ocean\"],\n          color = primary_palette[\"ocean\"]) +\n  \n  # Load Keswick Dam\n  geom_sf(data = dams,\n          color = primary_palette[\"crimson\"]) +\n  \n  # Add river annotation\n  annotate(\"text\",\n           x = 1, y = 1, \n           label = \"The Sacramento River\",\n           color = primary_palette[\"cascade\"],\n           fontface = \"bold\",\n           size = 2.5) +\n  \n  # Define Albers Equal Area proj for California\n  coord_sf(crs = st_crs(3311)) +\n  \n  # Define theme\n  theme_void() + \n  theme(\n    plot.background = element_rect(fill = primary_palette[\"ocean\"])\n  )\n\n# ---- Create The Donut Charts ----\n\n# Define chinook population proportion, based on 2023 and 1970 pop estimates\npop_prop &lt;- data.frame(\n  category = c(\"Remaining\", \"Historic Pop\"),\n  value = c(5, 95)\n)\n\n# Create habitat donut chart\nggplot(pop_prop, aes(x = 2,  # Set x to a constant value\n                     y = value,\n                     fill = category)) +\n  \n  geom_bar(stat = \"identity\",\n           width = 1,\n           color = \"white\") +  # Bar borders\n  \n  coord_polar(\"y\", start = 0) +  # Transform to polar coordinates\n  \n  # Define the width of the donut\n    xlim(0.2,\n       2.5) +\n  \n  scale_fill_manual(values = c( \"#F94B74\", \"#65D8FE\")) +\n  \n  labs(title = \"Only 5% of Historic Population Remains\") +\n  \n  theme_void() +\n  \n  theme(\n    plot.title = element_text(face = \"bold\",\n                              color = primary_palette[\"white\"],\n                              size = 16,\n                              hjust = 0.5),\n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    plot.background = element_rect(fill = primary_palette[\"ocean\"],\n                                   color = NA),\n    legend.position = \"none\"\n  )\n\n# Define habitat proportion, based estimates from CalFish and Boydstun 2001\npop_prop &lt;- data.frame(\n  category = c(\"Remaining\", \"Historic Pop\"),\n  value = c(10, 90)\n)\n\n# Create population donut chart\nggplot(pop_prop, aes(x = 2,\n                     y = value,\n                     fill = category)) +\n    \n  geom_bar(stat = \"identity\",\n           width = 1,\n           color = \"white\") +\n    \n  coord_polar(\"y\", start = 0) +\n\n  # Define width and height of chart\n  xlim(0.2,\n       2.5) +\n    \n  scale_fill_manual(values = c( \"#F94B74\", \"#65D8FE\")) +\n    \n  labs(title = \"90% of historic spawning habitat\\nis no longer accessible\") +\n    \n  theme_void() +\n\n  theme(\n    plot.title = element_text(face = \"bold\",\n                              color = primary_palette[\"white\"],\n                              size = 16,\n                              hjust = 0.5),\n    \n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    plot.background = element_rect(fill = primary_palette[\"ocean\"],\n                                   color = NA),\n    legend.position = \"none\"\n  ) \n\n# ---- Create The Stream Chart ----\n\n# Clean and filter data for winter-run chinook\nwinter_salmon &lt;- salmon_ca |&gt;\n    filter(c_name == \"Chinook\") |&gt;\n    filter(r_timing == \"Winter-run Chinook\") |&gt;\n    dplyr::group_by(y_end,\n                    r_timing) |&gt;\n    dplyr::summarise(abun_estimate = sum(abun_estimate, na.rm = TRUE),\n                     .groups = \"drop\") \n\n\n# Create stream chart for salmon abundance\n\n# Define plot\nggplot(data = winter_salmon,\n       aes(x = y_end,\n           y = abun_estimate,\n           fill = r_timing)) +\n    \n    # Add stream geometry for salmon abundance over time\n    geom_stream(type = \"ridge\") +\n    \n    # Define color scheme\n    scale_fill_manual(values = \"#65D8FE\") +\n    \n    # Label y axis with K for thousands\n    scale_y_continuous(labels = scales::label_number(scale = 1e-3,\n                                                     suffix = \"K\"),\n                        breaks = c(10e3, 30e3, 50e3),\n                        expand = c(0, 0)) +\n  \n    # Edit x axis labels and breaks\n    scale_x_continuous(breaks = seq(1970, 2030, by = 10),\n                       expand = c(0,0)) +\n    \n    # ---- Add annotations ----\n    \n    # 1994 label for pop below 200 ----\n            # Add red arrow for 1994 - population less than 200 ----\n            annotate(\"text\",\n                     x = 1994, y = 5000, \n                     label = \"↓\",\n                     color = primary_palette[\"salmon\"],\n                     size = 1) +\n            \n            # Add population text annotation\n            annotate(\"text\", x = 1994, y = 11500, \n                     label = \"1994\\nPopulation Drops\\nto less than 200\",\n                     color = primary_palette[\"white\"],\n                     size = 3,\n                     fontface = \"bold\") +\n            \n            # Add endangered text annotation\n            annotate(\"text\", x = 1993, y = 8000, \n                     label = \"Put on the endangered\\n species list\",\n                     color = primary_palette[\"white\"],\n                     size = 2.5) +\n  \n      # Add 2000 - 2005 bracket for increased delta release ----\n            # Add bracket from 2000 - 2005 ----\n            geom_segment(aes(x = 2000, xend = 2000,\n                             y = 9500, yend = 11000),\n                         color = primary_palette[\"cascade\"],\n                         size = .25,\n                         linetype = \"dashed\") +\n            \n            geom_segment(aes(x = 2005, xend = 2005,\n                             y = 9500, yend = 11000),\n                         color = primary_palette[\"cascade\"],\n                         size = .25,\n                         linetype = \"dashed\") +\n            \n            geom_segment(aes(x = 2000, xend = 2005,\n                             y = 11000, yend = 11000),\n                         color = primary_palette[\"cascade\"],\n                         size = .25,\n                         linetype = \"dashed\") +  \n  \n            # add text annotation for 20% increase\n            annotate(\"text\",\n                       x = 2002.5, y = 13500, \n                       label = \"2000-2005\\n20% Increase\\nin Delta Release\",\n                       color = primary_palette[\"white\"],\n                       size = 2.5) +\n  \n      # Add 2018 label for battle creek ----\n             # Add arrow\n              geom_segment(aes(x = 2018, xend = 2018,\n                             y = 0, yend = 7500),\n                         color = primary_palette[\"cascade\"],\n                         size = .25,\n                         linetype = \"dashed\") +\n            \n            # Add annotation for battle creek\n            annotate(\"text\",\n                     x = 2018, y = 10000, \n                     label = \"2018\\n20k juvenile Chinook\\nreleased in Battle Creek\",\n                     color = primary_palette[\"white\"],\n                     size = 2.5) +\n  \n   # Add 2021 label for eggs lost ----\n             # Add arrow\n              geom_segment(aes(x = 2021, xend = 2021,\n                             y = 0, yend = 17000),\n                         color = primary_palette[\"cascade\"],\n                         size = .25,\n                         linetype = \"dashed\") +\n            \n            # Add annotation for battle creek\n            annotate(\"text\",\n                     x = 2020, y = 20000, \n                     label = \"2021\\nAn estimated 75%\\nof eggs were lost due\\nto high river temperatures\",\n                     color = primary_palette[\"white\"],\n                     size = 2.5) +\n    \n    labs(\n        title = \"In the 90s winter-run Chinook narrowly avoided extinction\",\n        y = \"Estimated\\nWinter-Run\\nChinook\\nAdult\\nPopulation\",\n        fill = \"Return Type\") +\n    \n  # Define theme\n  theme_minimal() +\n  theme(\n        plot.title = element_text(face = \"bold\",\n                                  color = primary_palette[\"white\"],\n                                  size = 12),\n        axis.text.x = element_text(size = 10,\n                                   face = \"bold\",\n                                   color = primary_palette[\"white\"]),\n        axis.text.y = element_text(size = 10,\n                                   face = \"bold\",\n                                   color = primary_palette[\"white\"]),\n        axis.title.y = element_text(size = 11,\n                                    face = \"bold\",\n                                    color = primary_palette[\"white\"],\n                                    angle = 0,\n                                    vjust = .5),\n        axis.title.x = element_blank(),\n        panel.grid = element_blank(),\n        axis.line.x = element_line(color = primary_palette[\"white\"], size = 0.5),\n        axis.ticks.x = element_line(color = primary_palette[\"white\"], size = 0.5),\n        axis.ticks.y = element_blank(),\n        plot.background = element_rect(fill = primary_palette[\"ocean\"],\n                                       color = NA),\n        legend.position = \"none\"\n        ) \n    \n# ---- Create the Heatmap ----\n\n# Calculate average and max daily stream temperature\ndaily_avg_temp_kes20 &lt;- kes_stream_temp |&gt;\n  group_by(day_of_year,\n           year) |&gt;\n  summarise(avg_temp = mean(value, na.rm = TRUE),\n            max_temp = max(value, na.rm = TRUE)) |&gt;\n  ungroup()\n\n# Create heatmap of Keswick stream temperature by day of year\nggplot(daily_avg_temp_kes20,\n       aes(x = day_of_year,\n           y = factor(year), # facet by year\n           fill = max_temp)) +\n    \n  geom_tile() +\n    \n coord_cartesian(clip = \"off\") +\n     \n  # Add a box around April 15 to August 31\n  geom_rect(aes(xmin = as.numeric(strptime(\"04-15\", \"%m-%d\")$yday),\n                xmax = as.numeric(strptime(\"08-31\", \"%m-%d\")$yday),\n                ymin = -Inf,\n                ymax = Inf),\n            color = primary_palette[\"light-blue\"],\n            fill = NA,\n            size = 1.5) +\n    \n  # Add a small rectangle below the spawning season dates\n  geom_rect(aes(xmin = as.numeric(strptime(\"04-15\", \"%m-%d\")$yday),\n                xmax = as.numeric(strptime(\"08-31\", \"%m-%d\")$yday),\n                ymin = 0,\n                ymax = .4),\n            fill = primary_palette[\"light-blue\"],\n            color = primary_palette[\"light-blue\"],\n            size = 1) +\n    \n  # Label spawning season\n  geom_text(aes(x = (as.numeric(strptime(\"04-15\", \"%m-%d\")$yday + 1) + \n                     as.numeric(strptime(\"08-31\", \"%m-%d\")$yday + 1)) / 2,\n                y = 0.25,\n                label = \"Spawning Season: Apr. 1 - Aug. 31\"),\n            color = primary_palette[\"ocean\"],\n            fontface = \"bold\",\n            size = 3) +\n    \n    scale_fill_gradientn(colors = temp_palette) +\n    scale_y_discrete(limits = rev(levels(factor(daily_avg_temp_kes20$year)))) +  \n    \n  labs(title = \"Daily maximum river temperature below Keswick Dam.\") +\n  theme_void() +\n  theme(plot.title = element_text(color = primary_palette[\"white\"],\n                                  face = \"bold\",\n                                  size = 16),\n        axis.text.y = element_text(color = primary_palette[\"white\"],\n                                   face = \"bold\",\n                                   size = 20), \n        axis.ticks.y = element_blank(),\n        plot.background = element_rect(fill = primary_palette[\"ocean\"],\n                                       color = NA),\n        legend.text = element_text(color = primary_palette[\"white\"],\n                                   size = 12),\n        legend.title = element_blank(),\n        legend.position = \"top\",\n        legend.justification = c(1, 1))\n\n# ---- Create stacked bar chart ----\n\n# Calculate days exceeding the threshold of 53.5°F\nthreshold &lt;- 53.5 # Define threshold\n\n# Flag days where any hourly observation exceeds the threshold\ndaily_exceedance &lt;- kes_stream_temp |&gt;\n  group_by(year, month, obs_date) |&gt;\n  summarise(exceed_threshold = as.integer(any(value &gt; threshold,\n                                              na.rm = TRUE))) |&gt;\n  ungroup()\n\n# Filter data for spawning season (April - August)\nspawning_exceedance &lt;- daily_exceedance |&gt;\n  filter(month %in% c(\"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\")) |&gt; \n  group_by(year) |&gt;\n  summarise(exceed_days = sum(exceed_threshold),\n            total_days = n()) |&gt;\n  mutate(non_exceed_days = total_days - exceed_days) |&gt;\n  pivot_longer(cols = c(exceed_days, non_exceed_days), \n               names_to = \"exceed_type\", \n               values_to = \"days\") |&gt;\n  mutate(proportion = days / total_days)\n\n\n# Create Stacked bar chart showing proportion per year that exceeded or didn't\nggplot(spawning_exceedance, aes(y = factor(year,\n                                           levels = rev(unique(year))), \n                                x = proportion, \n                                fill = exceed_type)) +\n  geom_bar(stat = \"identity\",\n           color = primary_palette[\"white\"],\n           size = 0.5) +\n  \n  # Add labels for percentage\n  geom_text(aes(label = ifelse(round(proportion * 100) &gt;= 15, \n                               paste0(round(proportion * 100), \"%\"), \n                               \"\")),\n            position = position_stack(vjust = 0.5), \n            color = primary_palette[\"white\"], \n            fontface = \"bold\",\n            size = 12) +\n  \n  scale_fill_manual(name = \"Temperature Status\", \n                    values = c(\"exceed_days\" = \"#F94B74\", \n                               \"non_exceed_days\" = \"#20566E\")) +\n \n  scale_x_continuous(labels = scales::percent) +\n\n  labs(title = \"Percent of spawning season days that met and exceeded the threshold\") +\n    \n  theme_minimal() +\n    \n  theme(plot.title = element_text(face = \"bold\",\n                                  color = primary_palette[\"white\"],\n                                  size = 20,\n                                  hjust = 0.5),\n        \n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        panel.grid = element_blank(),\n        axis.ticks = element_blank(),\n        plot.background = element_rect(fill = primary_palette[\"ocean\"],\n                                       color = NA),\n        legend.position = \"none\")\n\n\n\n\n\nFinal Infographic:\n\n\n\n\n\n\nAcknowledgments:\nThis assignment was created for UCSB MEDS, EDS 240 - Data Visualization & Communication. Thank you to our professor Sam Shanny-Csik and teaching assistants Annie Adams and Sloane Stephenson for their wisdom and support throughout the class.\n\n\nReferences:\nCalifornia Department of Education. (n.d.). California State Boundary. Retrieved from https://hub.arcgis.com/datasets/ca7b47512a2a442fbfa039bded0b6eaf_0/explore?uiVersion=content-views\nNOAA Fisheries. (n.d.). Salmon Critical Habitat Maps and GIS Data - West Coast Region. Retrieved from https://www.fisheries.noaa.gov/resource/map/critical-habitat-maps-and-gis-data-west-coast-region\nNOAA Fisheries & California Department of Fish and Wildlife. (n.d.). Winter-run Chinook Salmon Habitat Boundary [ds800]. Retrieved from https://apps.wildlife.ca.gov/bios6/?al=ds800\nCalifornia Open Data Portal. (n.d.). i17 California Jurisdictional Dams Dataset. Retrieved from https://data.ca.gov/dataset/i17-california-jurisdictional-dams\nNOAA Fisheries. (n.d.). Recovery through Reintroductions: California’s Central Valley Salmon. Retrieved from https://www.fisheries.noaa.gov/west-coast/endangered-species-conservation/recovery-through-reintroductions-californias-central-valley-salmon\nThe Nature Conservancy. (n.d.). Statewide Status of Salmon Species in California. Retrieved from https://casalmon.org/statewide-status/#all-species\nCalifornia Department of Water Resources. (n.d.). California Data Exchange Center: Stream Temperature Data. Retrieved from https://cdec.water.ca.gov/dynamicapp/wsSensorData"
  },
  {
    "objectID": "blogs/2024-12-02-thomas-fire/index.html",
    "href": "blogs/2024-12-02-thomas-fire/index.html",
    "title": "A Step-By-Step Analysis of the Impacts of the Thomas Fire",
    "section": "",
    "text": "Source: NASA Earth Observatory\nThe Thomas Fire, which ignited on December 4, 2017, burned approximately 281,893 acres in Ventura and Santa Barbara counties, making it one of the largest and most devastating wildfires in California’s history. The fire left behind extensive burn scars with widespread loss of vegetation and unstable soil, leaving the region vulnerable to erosion. In 2018, heavy rains caused massive flooding and debris flows, which resulted in catastrophic damage and tragic loss of life.\nHere I share my beginner’s Python exercise exploring some of the impacts of the Thomas Fire using satellite imagery and air quality data."
  },
  {
    "objectID": "blogs/2024-12-02-thomas-fire/index.html#part-1.-visualizing-the-thomas-fire-burn-scar-using-false-color",
    "href": "blogs/2024-12-02-thomas-fire/index.html#part-1.-visualizing-the-thomas-fire-burn-scar-using-false-color",
    "title": "A Step-By-Step Analysis of the Impacts of the Thomas Fire",
    "section": "Part 1. Visualizing the Thomas Fire burn scar using false color",
    "text": "Part 1. Visualizing the Thomas Fire burn scar using false color\nSatellite data can be used to visualize landscapes beyond what the bare eye can see. In this section I utilize true color and false color settings to visualize the Thomas Fire burn area.\nSome of the technical highlights for this section include working with satellite imagery with rioxr, working with geospatial data with geopandas, and creating a map with matplotlib.\n\nStep 1A: Set up my workspace\nTo begin work this exercise I set up my workspace by loading in the libraries that are necessary for the analysis:\n\n# ---- Load libraries ----\nimport os # interacting with operating system\nimport numpy as np # numeric computing\nimport matplotlib.pyplot as plt # data visualization\nimport pandas as pd # general data wrangling\nimport geopandas as gpd # geospatial data wrangling\nfrom shapely.geometry import Polygon # creating and manipulating geometric objects\nfrom pyproj import CRS # crs management\nimport rioxarray as rioxr # working with raster data\nimport xarray as xr # working with arrays\n\nThen I configure my working environment and establish a directory structure for managing the project data.\n\n# ---- Set working environment ----\n\n# Set anaconda environment\nos.environ['PROJ_LIB'] = '/opt/anaconda3/share/proj'\n\n# Set up a root path directory\nroot = os.path.join('/',\n                  'courses',\n                  'EDS220',\n                  'data',\n                  'hwk4_landsat_data')\n\n\n\nStep 1B: Import & prepare data\nNext it’s time to read in my project data.\nThe thomas_fire.shp is a shapefile containing the fire perimeter for the Thomas Fire in 2017. It is subset of a CAL FIRE dataset with historical boundaries for fires (including both natural and prescribed fires) in the state of California. The dataset has a good record of past large fires but is not complete and may be missing some fires. I subset the Thomas Fire from the CAL FIRE dataset for this exercise, check out this notebook on GitHub if you’d like to see how I did this.\nThe landsat.nc dataset is an image from Landsat Collection 2 Level-2, from the Microsof Planetary Computer data catalogue. Landsat Collection 2 Level-2 Science Products consist of atmospherically corrected surface reflectance and surface temperature image data. Collection 2 Level-2 Science Products are available from August 22, 1982 to present. It is accessed through my university but it can also be accessed on USGS Earth Explorer.\n\n# ---- Import data ----\n\n# Import Thomas Fire boundary\nthomas_fire = gpd.read_file(\"data/thomas_fire/thomas_fire.shp\")\n\n# Import landsat .nc file\nfp = os.path.join(root,'landsat8-2018-01-26-sb-simplified.nc')\nlandsat = rioxr.open_rasterio(fp)\n\nNext, I will check the coordinate reference systems (CRS) of the landsat and thomas_fire datasets to ensure 1) the CRS aligns with the needs of the project and 2) the datasets share the same crs, allowing for proper overlay and analysis.\n\n# ---- Check CRS information ----\n\n# Check if landsat is projected \nprint(\"Is the landsat CRS projected?\", landsat.rio.crs.is_projected)\n\n# Check if thomas fire is projected \nprint(\"Is the thomas_fire CRS projected?\", thomas_fire.crs.is_projected)\n\n# Check if crs match\nprint(\"Do the crs match?\", landsat.rio.crs == thomas_fire.crs)\n\n# Print the crs name\nprint(\"The landsat CRS is:\", landsat.rio.crs)\n\n# Print the crs name\nprint(\"The thomas fire CRS is:\", thomas_fire.crs.name)\n\nIs the landsat CRS projected? True\nIs the thomas_fire CRS projected? True\nDo the crs match? False\nThe landsat CRS is: EPSG:32611\nThe thomas fire CRS is: WGS 84 / Pseudo-Mercator\n\n\nMy analysis found that the landsat and thomas_fire crs were both projected, but did not match. This means that we need to reproject the data to match. So next, I’m going to reproject the thomas_fire CRS is to match that landsat CRS which is EPSG 32611 or WGS 84 / UTM Zone 11N.\n\n# ---- Reproject CRS ----\n\n# Reproject to landsat crs\nthomas_fire = thomas_fire.to_crs(landsat.rio.crs)\n\n# Check if crs match\nprint(\"Do the crs match?\", landsat.rio.crs == thomas_fire.crs)\n\nDo the crs match? True\n\n\n\n\nStep 1C. Explore the Data\nNow that I have my data set up. I’m interested in learning more about the landsat data that I’ll be using. In the next code chunk, I explore different characteristics about my data.\n\n# ---- Explore data ----\n\n# Check out landsat data dimensions\nprint('Size of landsat dimensions: ', dict(landsat.sizes))\n\n# Check out the shape of the landsat data\nprint(\"The shape of Landsat is\", landsat.rio.shape)\n\n# Check out the variables of the landsat data\nprint(\"Variables in the Landsat data:\", list(landsat.data_vars))\n\n# Check out the original coordiantes of the landsat data\nprint(\"Variables in the Landsat data:\", list(landsat.coords))\n\nSize of landsat dimensions:  {'y': 731, 'x': 870, 'band': 1}\nThe shape of Landsat is (731, 870)\nVariables in the Landsat data: ['red', 'green', 'blue', 'nir08', 'swir22']\nVariables in the Landsat data: ['y', 'x', 'band', 'spatial_ref']\n\n\nIt looks like the landsat data is 870 by 731 pixels. It has has a total of a band length of 1 and the list of variables in the dataset are ‘red’, ‘green’, ‘blue’, ‘nir08’, and ‘swir22’.\n\n\nStep 1D. Prepare the landsat data for visualization\nSince the landsat data only has one band, I’m going to simplify it by dropping the dimensions that so that it’s easier to work with. To do this, I’ll use .squeeze() and .drop_vars() to remove the band length of 1.\n\n# Remove length 1 dimension (band) and reassign\nlandsat = landsat.squeeze().drop_vars('band', errors='ignore')\n\n# Check updated dimensions of the coordinates\nprint(landsat.dims, landsat.coords)\nprint('Size of landsat dimensions: ', dict(landsat.sizes))\n\nFrozen({'y': 731, 'x': 870}) Coordinates:\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n    spatial_ref  int64 0\nSize of landsat dimensions:  {'y': 731, 'x': 870}\n\n\nI can see that the extra dimension has been removed from the data! Yipee!\n\n\nStep 1E. Visualize the Landsat data in true color\nNow I want to take a look at the landsat data. First off, it would be helpful to see the study area in “natural-color” also known as “true-color’, so that I can get a sense of what the terrain looks like. True-color is a term used by remote sensing experts to describe displaying a satellite image in colors similar to what most people would naturally see with their bare eyes. This means plotting the ‘red’, ‘green’, and ‘blue’ bands in their respective order.\nHere I plot the landsat image in true color (R-G-B) to visualize what the burn area looks like over the region.\n\n# Plot the landsat bands in true color\nlandsat[['red', 'green', 'blue']].to_array().plot.imshow()\n\nplt.title(\"Thomas Fire Study Area\") # Add title\nplt.axis('off') # Removing axis labels and ticks\nplt.show() # Display the plot\n\n\n\n\n\n\n\n\nHmmmmm…. something here doesn’t look right! The map shows up in black and white when I was expecting it to look like a true color image.\nThis is because the clouds in the image introduce extreme, outlier values. By setting the robust = True parameter in the in .imshow(), I adjust the color scale based on the 2nd and 98th percentiles of the values, which helps minimize the influence of these outliers. This allows the color ramp to more accurately reflect the distribution of values where most of the data lies, providing a clearer representation of the underlying true color patterns in the image.\n\n# Plot the landsat bands in true color\nlandsat[['red', 'green', 'blue']].to_array().plot.imshow(robust=True) # scale on the 2 & 98th percentiles\n\nplt.title(\"Thomas Fire Study Area\") # Add title\nplt.axis('off') # Remove axis labels and ticks\nplt.show() # Display the plot\n\n\n\n\n\n\n\n\nAhhh, much better! Now this is more like what I was expecting to see in my true color image.\nThis map helps me get a feel for what the landscape looks like, but it is still difficult for me to distinguish between landcover like healthy vegetation and bare ground, since the green, brown, and tan colors don’t contrast strongly.\n\n\nStep 1F. Visualize the Landsat data in false\nNext, I’d like to see if I can visualize the burned area better by integrating other bands of the landsat image in a ‘false-color’ composite. False-color images use at least one wavelength outside the visible range, like near-infrared, which can highlight things that we can’t see with our bare eyes.\nVegetation strongly reflects in the near-infrared (NIR) and short-wave infrared (SWIR) bands and absorbs in the red, so visualizing these bands can help me distinguish between healthy vegetated from bare ground. For this reason, I will create a false-color composite using the SWIR, NIR, and red bands (SWIR-NIR-R) to better distinguish between vegetation and the Thomas Fire burned area.\n\n# Select and plot swir, nir, and red variables\nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True) # scale on the 2 & 98th percentiles\n\nplt.title(\"Thomas Fire Study Area\") # Add title\nplt.axis('off') # Remove axis labels and ticks\nplt.show() # Display the plot\n\n\n\n\n\n\n\n\nThat worked! Now I can much more easily distinguish between vegetation (green) and bare ground or stressed vegetation (tan and red). In the bottom right, I can make out a large irregular red shape which lines up with the area that the Thomas Fire burned.\n\n\nStep 1G. Overlay the Thomas Fire boundary on the false color image\nNow my final step is to overlay the false-color image with the Thomas Fire boundary to see how well the composite image picks up the burned area.\n\n# ---- Create Map of Thomas Fire Boundary on False Color Image ----\n\n# Select false color SWIR-NIR-red bands and set to array\nfalse_color = landsat[['swir22', 'nir08', 'red']].to_array()\n\n# Set aspect ratio for map\nfire_aspect_ratio = landsat.rio.shape[1] / landsat.rio.shape[0]\n\n# Set figure parameters\nfig, ax = plt.subplots(figsize=(6, 6*fire_aspect_ratio))\n\n# Add false color imagery for background\nfalse_color.plot.imshow(ax = ax,\n                        robust=True) \n# Add thomas fire boundary\nthomas_fire.boundary.plot(ax=ax,\n                color = \"#780B12\",\n                linewidth = 0.6,\n                alpha = 0.8)\n# Add title\nax.set_title('Thomas Fire Boundary (2017)         Southern California',\n            color=\"#222626\", \n            fontsize=12)\n\nax.axis('off') # Remove axis labels and ticks\nplt.legend(thomas_fire,loc='upper right', labels = [\"Thomas Fire\"]) # add legend\nplt.show() # Display the plot\n\n\n\n\n\n\n\n\nThis map shows the burn perimeter, outlined in dark red, for the Thomas Fire which occured in 2017. The boundary is overlayed onto a false color satellite image of the region, combining short-wave infrared (SWIR), near-infrared (NIR), and red bands to highlight vegetation (green) versus the burn scar (red). This overlay shows that the boundary of the burn scar aligns well with area in red in false color composite. When looking back at the true color image, this false-color map demonstrates the ability of the false color images to pick out land surface characteristics beyond what our bare eyes can see.\n\n# Display all columns in preview\npd.set_option(\"display.max.columns\", None)"
  },
  {
    "objectID": "blogs/2024-12-02-thomas-fire/index.html#part-2.-timeseries-analysis-of-the-thomas-fires-impact-on-regional-air-quality",
    "href": "blogs/2024-12-02-thomas-fire/index.html#part-2.-timeseries-analysis-of-the-thomas-fires-impact-on-regional-air-quality",
    "title": "A Step-By-Step Analysis of the Impacts of the Thomas Fire",
    "section": "Part 2. Timeseries Analysis of the Thomas Fire’s Impact on Regional Air Quality",
    "text": "Part 2. Timeseries Analysis of the Thomas Fire’s Impact on Regional Air Quality\n\nSource: NASA Earth Observatory\nIn the next section, I explore the impact of the Thomas Fire on air quality using Air Quality Index (AQI) data. The U.S Environmental Protection Agency’s (EPA) AQI is a standard scale that measures air quality and its associated health impacts.\n\nSource: U.S. EPA\nThis table shows the six AQI levels, where a higher AQI corresponds with more severe health concerns.\nFor this exercise I use regional AQI data from the U.S. EPA. Some of the technical highlights for this section include data wrangling with pandas , numerical processing with numpy, and time series data visualization with matplotlib.\n\nStep 2A: Set up my workspace\nTo begin, I set up my workspace by loading in the libraries that are necessary for the analysis:\n\n# Import libraries\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nStep 2B: Import & prepare data\nNow it’s time to read in my project data!\nThe aqi_17.csv and aqi_18.csv are files containing AQI observations from air quality monitors for the 2017 and 2018 years, respectively, in the Thomas Fire region. I read in the data directly from the Air Quality Index (AQI) Basics on the AirNow.gov portal.\n\n# Read in AQI data\naqi_17 = pd.read_csv('data/aqi/daily_aqi_by_county_2017.zip', compression='zip')\naqi_18 = pd.read_csv('data/aqi/daily_aqi_by_county_2018.zip', compression='zip')\n\n\n\nStep 2C. Explore the Data\nNow that I have my data set up, I’m interested in learning more about the AQI data that I’ll be using. In the next code chunk, I explore different characteristics about my data.\n\n# Look at the head, first 5 entries, of the AQI 2017 data\naqi_17.head()\n\n\n\n\n\n\n\n\nState Name\ncounty Name\nState Code\nCounty Code\nDate\nAQI\nCategory\nDefining Parameter\nDefining Site\nNumber of Sites Reporting\n\n\n\n\n0\nAlabama\nBaldwin\n1\n3\n2017-01-01\n28\nGood\nPM2.5\n01-003-0010\n1\n\n\n1\nAlabama\nBaldwin\n1\n3\n2017-01-04\n29\nGood\nPM2.5\n01-003-0010\n1\n\n\n2\nAlabama\nBaldwin\n1\n3\n2017-01-10\n25\nGood\nPM2.5\n01-003-0010\n1\n\n\n3\nAlabama\nBaldwin\n1\n3\n2017-01-13\n40\nGood\nPM2.5\n01-003-0010\n1\n\n\n4\nAlabama\nBaldwin\n1\n3\n2017-01-16\n22\nGood\nPM2.5\n01-003-0010\n1\n\n\n\n\n\n\n\n\n# Look at the head, first 5 entries, of the AQI 2018 data\naqi_18.head()\n\n\n\n\n\n\n\n\nState Name\ncounty Name\nState Code\nCounty Code\nDate\nAQI\nCategory\nDefining Parameter\nDefining Site\nNumber of Sites Reporting\n\n\n\n\n0\nAlabama\nBaldwin\n1\n3\n2018-01-02\n42\nGood\nPM2.5\n01-003-0010\n1\n\n\n1\nAlabama\nBaldwin\n1\n3\n2018-01-05\n45\nGood\nPM2.5\n01-003-0010\n1\n\n\n2\nAlabama\nBaldwin\n1\n3\n2018-01-08\n20\nGood\nPM2.5\n01-003-0010\n1\n\n\n3\nAlabama\nBaldwin\n1\n3\n2018-01-11\n25\nGood\nPM2.5\n01-003-0010\n1\n\n\n4\nAlabama\nBaldwin\n1\n3\n2018-01-14\n33\nGood\nPM2.5\n01-003-0010\n1\n\n\n\n\n\n\n\n\n# Explore the spread of AQI observation values\nprint(\"Max AQI for 2017 =\", aqi_17['AQI'].max())\nprint(\"Min AQI for 2017 =\", aqi_17['AQI'].min())\nprint(\"Max AQI for 2018 =\", aqi_18['AQI'].max())\nprint(\"Min AQI for 2018 =\", aqi_18['AQI'].min())\n\n# Explore data types\nprint(aqi_17.dtypes)\nprint(aqi_18.dtypes)\n\nMax AQI for 2017 = 3767\nMin AQI for 2017 = 0\nMax AQI for 2018 = 1051\nMin AQI for 2018 = 0\nState Name                   object\ncounty Name                  object\nState Code                    int64\nCounty Code                   int64\nDate                         object\nAQI                           int64\nCategory                     object\nDefining Parameter           object\nDefining Site                object\nNumber of Sites Reporting     int64\ndtype: object\nState Name                   object\ncounty Name                  object\nState Code                    int64\nCounty Code                   int64\nDate                         object\nAQI                           int64\nCategory                     object\nDefining Parameter           object\nDefining Site                object\nNumber of Sites Reporting     int64\ndtype: object\n\n\nIn the data exploration above, I used dtypes to see the data types for the different columns. I also used max and min to find the maximum and minimum AQI values for the datasets. Then I used value_counts to find the value counts of observations that fell in the different category ratings.\n\n\nStep 2D. Prepare the AQI data for visualization\nGreat, now I’m familiar with the AQI data that I’ll be using, it’s time to prepare it for visualization. Since aqi_17 and aqi_18 are saved as two separated data frames, I want to merge them together using the pd.concat() tool and store the output as aqi.\n\n# ---- Combine the datasets ----\n\n# Combine the two datasets using `pd.concat`\naqi = pd.concat([aqi_17, aqi_18])\n\nNow that we have combined the datasets, I want to take a look at what the new dataframe looks like.\n\n# ---- Explore the AQI data ----\n\n# Check out the head of the new df\nprint(aqi.head())\n\n# Check out column names\nprint(\"The column names area:\", list(aqi.columns))\n\n  State Name county Name  State Code  County Code        Date  AQI Category  \\\n0    Alabama     Baldwin           1            3  2017-01-01   28     Good   \n1    Alabama     Baldwin           1            3  2017-01-04   29     Good   \n2    Alabama     Baldwin           1            3  2017-01-10   25     Good   \n3    Alabama     Baldwin           1            3  2017-01-13   40     Good   \n4    Alabama     Baldwin           1            3  2017-01-16   22     Good   \n\n  Defining Parameter Defining Site  Number of Sites Reporting  \n0              PM2.5   01-003-0010                          1  \n1              PM2.5   01-003-0010                          1  \n2              PM2.5   01-003-0010                          1  \n3              PM2.5   01-003-0010                          1  \n4              PM2.5   01-003-0010                          1  \nThe column names area: ['State Name', 'county Name', 'State Code', 'County Code', 'Date', 'AQI', 'Category', 'Defining Parameter', 'Defining Site', 'Number of Sites Reporting']\n\n\nOooof! I do not like the look of the column names. Their naming convention is all over the place, with all those capitalizations and spaces. This can be annoying to work with in Python, so I’ll want to tidy it up so that it’s easier to work with!\nI want to change my column names to lower snake case, a popular naming convention that uses lowercase letters and underscores to separate words.\nIn this next code chunk, I’m going to convert the column names to lower snake case, a popular naming convention that uses lowercase letters and underscores to separate words. I’ll do this by selecting the columns using .columns, forcing all strings (text) to lower case using .lower(), and replacing all spaces ' 'with underscores '_' using .replace().\n\n# Simplify column names\naqi.columns = (aqi.columns # Selects column names of the aqi df\n                  .str.lower() # Forces text to lower case\n                  .str.replace(' ','_') # replaces spaces ' ' with underscores '_'\n                )\nprint(\"The column names are:\", list(aqi.columns)) # print the updated column names\n\nThe column names are: ['state_name', 'county_name', 'state_code', 'county_code', 'date', 'aqi', 'category', 'defining_parameter', 'defining_site', 'number_of_sites_reporting']\n\n\n\n\nStep 2E. Subset & prepare aqi data for timeseries visualization\nNow that I’ve cleaned up my data, it’s time to start some wrangling! I’m going to subset aqi to Santa Barbara county by selecting the column where 'county_name' is Santa Barbara. I also want to remove some of the column names that I won’t be using for my visualization including 'state_name', 'county_name', 'state_code', and 'county_code'.\n\n# ---- Subset `aqi` data for visualization ----\n\n# Subset AQI data to Santa Barbara county\naqi = aqi[aqi['county_name'] == 'Santa Barbara']\n\n# Remove the 'state_name', 'county_name', 'state_code' and 'county_code' columns from aqi_sb\naqi = aqi.drop(['state_name', 'county_name', 'state_code', 'county_code'], axis = 1)\n\nAwesome, now my data is subset to my area of interest!\nNext I have want to prepare the date column so that I can visualize aqi as a time series.\nLet me check out how the date column is being stored using type().\n\n# Check the type of date column\ndate_type = type(aqi.date)\ndate_type\n\npandas.core.series.Series\n\n\nIt looks like it is a pandas.core.series.Series, so I will want to update the date column to be a pandas.datetime object using pd.to_datetime().\n\n# Update the data column to datetime object\naqi.date = pd.to_datetime(aqi.date)\n\nAnd now lets set the 'date' column as the index for the aqi dataframe using .set_index.\n\n# Set date as the index of aqi_sb\naqi = aqi.set_index('date')\n\n\n\nStep 2F. Calculate 5-Day Rolling Average for AQI\nBefore I create the timeseries plot, I’d like to smooth out the data a bit by calculating the average over a rolling, 5-day window. In python we can do this using .rolling() by specifying the window length as '5D' and the type of calculation to be .mean().\n\n# Calculate AQI rolling average over 5 days\nrolling_average = (aqi['aqi']\n                   .rolling(window = '5D') # Specify window length\n                   .mean()) # Specify caluation as mean\nrolling_average\n\ndate\n2017-01-01    39.000000\n2017-01-02    39.000000\n2017-01-03    49.666667\n2017-01-04    45.750000\n2017-01-05    44.000000\n                ...    \n2018-12-27    41.200000\n2018-12-28    38.600000\n2018-12-29    38.200000\n2018-12-30    38.200000\n2018-12-31    38.800000\nName: aqi, Length: 730, dtype: float64\n\n\nNow, let’s add these calculations to the aqi dataframe.\n\n# Add calculated rolling averages as 'five_day_average' column\naqi[\"five_day_average\"] = rolling_average\n\naqi\n\n\n\n\n\n\n\n\naqi\ncategory\ndefining_parameter\ndefining_site\nnumber_of_sites_reporting\nfive_day_average\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2017-01-01\n39\nGood\nOzone\n06-083-4003\n12\n39.000000\n\n\n2017-01-02\n39\nGood\nPM2.5\n06-083-2011\n11\n39.000000\n\n\n2017-01-03\n71\nModerate\nPM10\n06-083-4003\n12\n49.666667\n\n\n2017-01-04\n34\nGood\nOzone\n06-083-4003\n13\n45.750000\n\n\n2017-01-05\n37\nGood\nOzone\n06-083-4003\n12\n44.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2018-12-27\n37\nGood\nOzone\n06-083-1025\n11\n41.200000\n\n\n2018-12-28\n39\nGood\nOzone\n06-083-1021\n12\n38.600000\n\n\n2018-12-29\n39\nGood\nOzone\n06-083-1021\n12\n38.200000\n\n\n2018-12-30\n41\nGood\nPM2.5\n06-083-1008\n12\n38.200000\n\n\n2018-12-31\n38\nGood\nOzone\n06-083-2004\n12\n38.800000\n\n\n\n\n730 rows × 6 columns\n\n\n\n\n\nStep 2E. Create AQI Time Series Plot for the Thomas Fire\nPhew! Now that I have the data all set up, it’s finally time to create a visualization. I’m going to plot aqi as a timeseries to explore how it was impacted by the Thomas Fire.\nIn this next code chunk, I’ll create the plot along with some custom formatting elements to help provide context for the figure using matplotlib(). I’ll set up 'date' on the x-axis and aqi and rolling_average on the y axis.\nI’ll add a vertical span plt.axvspan and annotation plt.annotate in transparent red to shade in the duration of the Thomas Fire. To indicate benchmarks for ‘unhealthy’, ‘very unhealthy’, and ‘hazardous’ aqi levels I’ll using plt.axhline and plt.text to add horizontal dashed lines.\nTo finish it off, I’ll incorporate custom layout settings including a title, axis labels, gridlines, and a legend.\n\n# Set figure size (wider plot)\nfig, ax = plt.subplots(figsize=(12, 6)) \n\n# ---- Add vertical and horizontal labels ----\n\n# Add vertical span indicating Thomas Fire duration\nplt.axvspan(pd.to_datetime('2017-12-04'), pd.to_datetime('2018-01-12'), color ='#A3210D', alpha = 0.05)\nplt.annotate('Thomas Fire', xy = (pd.to_datetime('2018-01-12'), 200), xytext = (pd.to_datetime('2018-01-12'), 230),\n             arrowprops=dict(arrowstyle = '-', color='#A3210D'), color='#A3210D', alpha=0.7)\n\n# Add dashed horizontal lines for AQI levels\nplt.axhline(y=151, color = 'orange', linestyle ='--', label ='_nolegend_', linewidth=1.5)\nplt.axhline(y=201, color = 'red', linestyle ='--', label ='_nolegend_', linewidth=1.5)\nplt.axhline(y=301, color = 'purple', linestyle ='--', label ='_nolegend_', linewidth=1.5)\n\n# Add text labels on the lines, moved right by a bit for better placement\nplt.text(aqi.index[0] + pd.Timedelta(days = 20), 151, 'Unhealthy', color ='orange', fontsize=10, verticalalignment='bottom')\nplt.text(aqi.index[0] + pd.Timedelta(days = 20), 201, 'Very Unhealthy', color ='red', fontsize=10, verticalalignment='bottom')\nplt.text(aqi.index[0] + pd.Timedelta(days = 20), 301, 'Hazardous', color ='purple', fontsize=10, verticalalignment='bottom')\n\n# ---- Plot AQI data ----\n\n# Plot AQI and 5-day average\nplt.plot(aqi.index, aqi.aqi, color='#80C4B4', label='Daily AQI')\nplt.plot(aqi.index, aqi.five_day_average, color='#596964', label='5-Day Average AQI')\n\n\n# ---- Customize layout ----\n\n# Title and Labels\nplt.title('Air Quality Index (AQI) Spike During the 2017 Thomas Fire in Santa Barbara County', fontsize=16)\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('AQI', fontsize=12)\n\n# Customize ticks\nplt.xticks(rotation =45, ha ='right')\nplt.yticks(range(0, 350, 50))\n\n# Add gridlines for better readability\nplt.grid(True, linestyle ='--', alpha = 0.2)\n\n# Add Legend\nplt.legend(bbox_to_anchor = (1.05, 1), loc = 'upper left', fontsize=10)\n\n# Show plot\nplt.tight_layout() # Minimizes padding\nplt.show()\n\n\n\n\n\n\n\n\nThis is a timeseries plot of aqi during the Thomas Fire in Santa Barbara County with date on the x-axis and aqi (light blue) and 5-day rolling average (dark green) on the y-axis. The figure shows a distinct spike in AQI in late 2017, during the Thomas Fire which burned from December 4, 2017 through January 12, 2018. At the start of the fire, the Daily AQI spiked above “Hazardous” AQI level, 300, which warns of emergency conditions where everyone, not just senstive populations, are likely to face health effects. The figure identifies an additional brief but significant spike in Santa Barbara, where the AQI rose above 200, which the EPA ranks as ‘Very Unhealthy’; this could suggest the occurance of another fire or event that contributed to the dramatic spike.\nThis timeseries is an effective way to tell the story of how fires can affect AQI and pose a significant risk to human health.\nThat’s it for my step-by-step analysis of the Thomas Fire. I hope that you enjoyed following along!\n\n\nReferences:\nCAL FIRE (2024) California Fire Perimeters (all) [Data file] Available from: https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436 Access date: 11/20/24\nUSGS (2024) Landsat Collection 2 Level-2 Surface Reflectance and Surface Temperature Products [Data file]. Available from: https://www.usgs.gov/landsat-missions/landsat-collection-2 Access date: 11/20/24\nEPA (2024) Daily AQI By County 2017 & 2018 [Data file] Available from: https://www.epa.gov/ Access date: 12/04/24\nCarmen Galaz García (2024) UCSB MEDS - 220 - Working With Environmental Datasets  [Source of Homework Assignment]. Course Website: https://meds-eds-220.github.io/MEDS-eds-220-course/ Access date: 11/20/24"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicole Pepper",
    "section": "",
    "text": "Hey there!\nI’m Nicole Pepper, a graduate student at the University of California, Santa Barbara, pursuing a Master’s in Environmental Data Science (MEDS) at the Bren School. My passion lies in leveraging spatial and diverse datasets to tackle pressing environmental challenges and inspire meaningful action. When I’m not wrangling data, you’ll find me enjoying the great outdoors, especially in the mountains. Take a look around to learn more about my work and the impact I’m striving to make.\n\n\nEducation\nUniversity of California Santa Barbara, Bren School of Environmental Science & Management Masters in Environmental Data Science (Expected June 2025)\nUniversity of California Los Angeles B.A. in Geography and Environmental Studies, Minor in Geospatial Information Systems & Technology (June 2018)\n\n\nProfessional Experience\nNASA Acres Program and Communications Coordinator (May 2023 - Present)\nColorado Mountain College GIS Teaching Assistant and Tutor (April 2020 - June 2024)\nYampa Valley Sustainability Council Geospatial Analyst & Internship Program Manager (April 2020 - May 2023)\n\n\nSkills\nGIS & Data Science:\n\nProgramming Languages: Python, R, JavaScript, BASH, Markdown\nSoftware: ESRI Suite Products (ArcGIS Pro, ArcGIS Online, & others), QGIS, RStudio, Visual Studio Code, Google Earth Engine\n\nGraphic Design & Production:\n\nGraphic Design: Adobe Illustrator, Affinity, Canva\nVideo & Audio Production: Adobe Premiere Pro\nWeb Design: Wordpress, SquareSpace, GitHub Pages, Quarto Markdown Websites"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blog",
    "section": "",
    "text": "A Look At Elevated Risk Factors for Avalanche Accidents in Colorado\n\n\nUCSB MEDS - EDS 222 - Stats for Environmental Data Science\n\n\n\nNicole Pepper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Upstream Battle For Winter-Run Chinook in California\n\n\nDesigning an Infographic in R\n\n\n\nNicole Pepper\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Step-By-Step Analysis of the Impacts of the Thomas Fire\n\n\n\nGIS\n\n\nRemote-Sensing\n\n\nMEDS\n\n\nPython\n\n\n\nUsing Satellite & Air Quality Data in Python\n\n\n\nNicole Pepper\n\n\nDec 4, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]